{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boris_extraction as boris\n",
    "import multirecording_spikeanalysis as spike\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "import matplotlib.patches as mpatches\n",
    "from itertools import combinations\n",
    "\n",
    "def hex_2_rgb(hex_color): # Orange color\n",
    "    rgb_color = tuple(int(hex_color[i:i+2], 16) / 255.0 for i in (1, 3, 5))\n",
    "    return rgb_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_this(thing_to_pickle, file_name):\n",
    "    \"\"\"\n",
    "    Pickles things\n",
    "    Args (2):   \n",
    "        thing_to_pickle: anything you want to pickle\n",
    "        file_name: str, filename that ends with .pkl \n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    with open(file_name,'wb') as file:\n",
    "        pickle.dump(thing_to_pickle, file)\n",
    "\n",
    "def unpickle_this(pickle_file):\n",
    "    \"\"\"\n",
    "    Unpickles things\n",
    "    Args (1):   \n",
    "        file_name: str, pickle filename that already exists and ends with .pkl\n",
    "    Returns:\n",
    "        pickled item\n",
    "    \"\"\"\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        return(pickle.load(file))\n",
    "\n",
    "\n",
    "def random_event_generator(start, stop, len_event, no_events):\n",
    "    total_duration = stop - start\n",
    "    possible_events = np.arange(int(total_duration / len_event))\n",
    "    pot_events = np.random.choice(possible_events, size = (no_events), replace = False)\n",
    "    pot_events = np.sort(pot_events)\n",
    "    events = []\n",
    "    for i in pot_events: \n",
    "        event_start = (start + (len_event * i)) * 1000\n",
    "        event_stop = (event_start + (len_event * 1000))\n",
    "        events.append(np.array([event_start, event_stop]))\n",
    "    return(np.array(events))\n",
    "\n",
    "def p2_create_random_array(times, order, len_event, no_events, media_duration):\n",
    "    media_duration = media_duration/1000\n",
    "    acquisition_array = random_event_generator(0, times[0], len_event, no_events)\n",
    "    exposure1_array = random_event_generator(times[3], times[4], len_event, no_events)\n",
    "    exposure2_array = random_event_generator(times[5], times[6], len_event, no_events)\n",
    "    exposure3_array = random_event_generator(times[7], media_duration, len_event, no_events)\n",
    "    order_arrays = [exposure1_array, exposure2_array, exposure3_array]\n",
    "    for i in range(len(order)):\n",
    "        if order[i] == 'familiar':\n",
    "            recall_array = order_arrays[i]\n",
    "        if order[i] == 'cagemate':\n",
    "            cagemate_array = order_arrays[i]\n",
    "        if order[i] == 'novel':\n",
    "            novel_array = order_arrays[i]\n",
    "    event_dict = {'acquisition': acquisition_array, 'recall': recall_array, 'cagemate':cagemate_array, 'novel': novel_array, \n",
    "                'exposure 0': acquisition_array, 'exposure 1':exposure1_array, 'exposure 2': exposure2_array, 'exposure 3': exposure3_array}\n",
    "\n",
    "    return event_dict\n",
    "\n",
    "def p2_create_array(boris_df, times, order, min_iti, min_bout):\n",
    "    familiarization_df = boris_df[(boris_df['Start (s)'] < times[0])]\n",
    "    if order[0] == 'familiar':\n",
    "        recall_df = boris_df[(boris_df['Start (s)'] > times[3]) & (boris_df['Start (s)'] < times[4])]\n",
    "        exposure1_df = recall_df\n",
    "    if order[0] == 'cagemate':\n",
    "        cagemate_df = boris_df[(boris_df['Start (s)'] > times[3]) & (boris_df['Start (s)'] < times[4])]\n",
    "        exposure1_df = cagemate_df\n",
    "    if order[0] == 'novel':\n",
    "        novel_df = boris_df[(boris_df['Start (s)'] > times[3]) & (boris_df['Start (s)'] < times[4])]\n",
    "        exposure1_df = novel_df\n",
    "    if order[1] == 'familiar':\n",
    "        recall_df = boris_df[(boris_df['Start (s)'] > times[5]) & (boris_df['Start (s)'] < times[6])]\n",
    "        exposure2_df = recall_df \n",
    "    if order[1] == 'cagemate':\n",
    "        cagemate_df = boris_df[(boris_df['Start (s)'] > times[5]) & (boris_df['Start (s)'] < times[6])]\n",
    "        exposure2_df = cagemate_df \n",
    "    if order[1] == 'novel':\n",
    "        novel_df = boris_df[(boris_df['Start (s)'] > times[5]) & (boris_df['Start (s)'] < times[6])]\n",
    "        exposure2_df = novel_df\n",
    "    if order[2] == 'familiar':\n",
    "        recall_df = boris_df[boris_df['Start (s)'] > times[7]]\n",
    "        exposure3_df = recall_df \n",
    "    if order[2] == 'novel':\n",
    "        novel_df = boris_df[boris_df['Start (s)'] > times[7]]\n",
    "        exposure3_df = novel_df\n",
    "    if order[2] == 'cagemate':\n",
    "        cagemate_df = boris_df[boris_df['Start (s)'] > times[7]]\n",
    "        exposure3_df = cagemate_df \n",
    "\n",
    "    acquisition_array = boris.get_behavior_bouts(familiarization_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    recall_array = boris.get_behavior_bouts(recall_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    cagemate_array = boris.get_behavior_bouts(cagemate_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    novel_array = boris.get_behavior_bouts(novel_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    exposure1_array = boris.get_behavior_bouts(exposure1_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    exposure2_array = boris.get_behavior_bouts(exposure2_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    exposure3_array = boris.get_behavior_bouts(exposure3_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    event_dict = {'acquisition': acquisition_array, 'recall': recall_array, 'cagemate':cagemate_array, 'novel': novel_array, \n",
    "                      'exposure 0': acquisition_array, 'exposure 1':exposure1_array, 'exposure 2': exposure2_array, 'exposure 3': exposure3_array}\n",
    "\n",
    "    return event_dict\n",
    "\n",
    "def p2_make_assignment(recording, subject, event_dict):\n",
    "    recording.event_dict = event_dict\n",
    "    recording.subject = subject\n",
    "\n",
    "def p2_camera_crash(boris_df1, boris_df2, times1, times2, order, media_duration, last_timestamp, min_iti, min_bout):\n",
    "    array_1 = p2_create_array(boris_df1, times1, order, min_iti, min_bout)\n",
    "    array_2 = p2_create_array(boris_df2, times2, order, min_iti, min_bout)\n",
    "    diff = (last_timestamp / 20000 * 1000) - (media_duration)\n",
    "    final_dict = {}\n",
    "    for event, times in array_1.items():\n",
    "        array_2[event] = array_2[event] + diff\n",
    "        new_array = np.concatenate([array_1[event], array_2[event]])\n",
    "        final_dict[event] = new_array\n",
    "    return final_dict\n",
    "    \n",
    "def p2_camera_crash_random(times1, times2, order, media_duration2, last_timestamp, len_event, no_events):\n",
    "    diff = (last_timestamp / 20000) - (media_duration2/1000)\n",
    "    times1 = np.array(times1)\n",
    "    times2 = np.array(times2)\n",
    "    times = [0,0,0,0,0,0,0,0]\n",
    "    for i in range(len(times1)):\n",
    "        if i == len(times1) - 1:\n",
    "            times[i] = times2[i] + diff\n",
    "        else:\n",
    "            if times1[i+1] != 0:\n",
    "                if i == len(times1) - 2:\n",
    "                    times[i] = times2[i] + diff\n",
    "                else:\n",
    "                    times[i] = times1[i]\n",
    "            else:\n",
    "                times[i] = times2[i] + diff\n",
    "    event_dict = p2_create_random_array(times, order, len_event, no_events, (last_timestamp/20))\n",
    "    return event_dict  \n",
    "\n",
    "\n",
    "def assign_dicts(ephys_collection, dict_dict):\n",
    "    \"\"\"\n",
    "    Assigns behavior dictionaries to recordings in an ephys collection\n",
    "\n",
    "    Args(2):\n",
    "        ephys_collection: an ephys collection class instance\n",
    "        dict_dict: dict, a dictionary of behavior event dictionaries\n",
    "            keys: recording names \n",
    "            values: behavior event dictionaries (keys: events, values: start stop times)\n",
    "    \"\"\"\n",
    "    collection = ephys_collection.collection\n",
    "    for recording, event_dict in dict_dict.items():\n",
    "        collection[recording].event_dict = event_dict\n",
    "\n",
    "        \n",
    "def create_collection(new=False):\n",
    "    if new:\n",
    "        phase2_collection = spike.EphysRecordingCollection(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\")\n",
    "        with open('phase2collection.pkl','wb') as file:\n",
    "                pickle.dump(phase2_collection, file)\n",
    "    else:\n",
    "        try:\n",
    "            with open('phase2collection.pkl', 'rb') as file:\n",
    "                phase2_collection = pickle.load(file)\n",
    "        except FileNotFoundError:\n",
    "            phase2_collection = spike.EphysRecordingCollection(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\")\n",
    "            with open('phase2collection.pkl','wb') as file:\n",
    "                pickle.dump(phase2_collection, file)\n",
    "    return phase2_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2_collection = create_collection()\n",
    "FCN_1_1 = phase2_collection.get_by_name('20230803_101331_1_merged.rec')\n",
    "CNF_1_1 = phase2_collection.get_by_name('20230817_100823_1_merged.rec')\n",
    "NFC_1_1 = phase2_collection.get_by_name('20230818_115728_1_merged.rec')\n",
    "\n",
    "NFC_1_2 = phase2_collection.get_by_name('20230804_141009_1_merged.rec')\n",
    "FCN_1_2 = phase2_collection.get_by_name('20230817_113746_1_merged.rec')\n",
    "CNF_1_2 = phase2_collection.get_by_name('20230803_141047_1_merged.rec')\n",
    "                                        \n",
    "FCN_1_4 = phase2_collection.get_by_name('20230804_121600_1_merged.rec')\n",
    "NFC_1_4 = phase2_collection.get_by_name('20230803_121318_1_merged.rec')\n",
    "CNF_1_4 = phase2_collection.get_by_name('20230818_133620_1_merged.rec')\n",
    "FCN_1_1.subject = '1.1'\n",
    "CNF_1_1.subject = '1.1'\n",
    "NFC_1_1.subject = '1.1'\n",
    "NFC_1_2.subject = '1.2'\n",
    "FCN_1_2.subject = '1.2'\n",
    "CNF_1_2.subject = '1.2'\n",
    "FCN_1_4.subject = '1.4'\n",
    "NFC_1_4.subject = '1.4'\n",
    "CNF_1_4.subject = '1.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #ONE VIDEO\n",
    "    #NEED TO DO ADD MEDIA DURAITONS FOR ALL VIDEOS REGARDLESS OF TIME CRASH \n",
    "    #20230817_100823_1.1_CNF\n",
    "    CNF_1_1_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230817_100823_1_merged.rec\\\\20230817_100823_1.1_CNF.xlsx\")\n",
    "    times_CNF_1_1 = [630, 633, 1226, 1228, 1526, 1531, 1828, 1832]\n",
    "    order_CNF_1_1 = ['cagemate', 'novel', 'familiar']\n",
    "    CNF_1_1_media_duration = 2131.233 * 1000\n",
    "    CNF_1_1_last_timestamp = CNF_1_1.timestamps_var[-1]\n",
    "\n",
    "\n",
    "    #TWO VIDEOS\n",
    "    #20230803_101331_1_FCN_1\n",
    "    FCN_1_1_1_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230803_101331_1_merged.rec\\\\20230803 101331 1.1.xlsx\")\n",
    "    FCN_1_1_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\Phase 2\\\\20230803_101331_1_merged.rec\\\\20230803 101331 1.2.xlsx\")\n",
    "    times_FCN_1_1_1 = [599, 603, 821, 0, 0, 0, 0, 99999999999]\n",
    "    times_FCN_1_1_2 = [0, 0, 374, 377, 671, 677, 970, 976]\n",
    "    order_FCN_1_1 = ['familiar', 'cagemate', 'novel']\n",
    "    FCN_1_1_media_duration = 1290.567 * 1000\n",
    "    FCN_1_1_last_timestamp = FCN_1_1.timestamps_var[-1]\n",
    "    FCN_1_1_media_duration1 = 821.200 * 1000\n",
    "\n",
    "\n",
    "    #20230818_115728_1.1_NFC\n",
    "    NFC_1_1_1_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230818_115728_1_merged.rec\\\\20230818 115728 1.1.xlsx\")\n",
    "    NFC_1_1_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230818_115728_1_merged.rec\\\\20230818 115728 1.2.xlsx\")\n",
    "    times_NFC_1_1_1 = [111, 0, 0, 0, 0, 0, 0, 99999999999]\n",
    "    times_NFC_1_1_2 = [527, 533, 1147, 1151, 1446, 1454, 1754, 1762]\n",
    "    order_NFC_1_1 = ['novel', 'familiar', 'cagemate']\n",
    "    NFC_1_1_media_duration = 2059.6 * 1000\n",
    "    NFC_1_1_last_timestamp = NFC_1_1.timestamps_var[-1]\n",
    "    NFC_1_1_media_duration1 = 115.667 * 1000\n",
    "\n",
    "    #20230704_141009_1.2_NFC8\n",
    "    NFC_1_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230804_141009_1_merged.rec\\\\20230804_141009_1.2_2t2bL_NFC.xlsx\")\n",
    "    times_NFC_1_2 = [600, 606, 1199, 1202, 1500, 1508, 1800, 1806]\n",
    "    order_NFC_1_2 = ['novel', 'familiar', 'cagemate']\n",
    "    NFC_1_2_media_duration  = 2106.100 * 1000\n",
    "    NFC_1_2_last_timestamp = NFC_1_2.timestamps_var[-1]\n",
    "\n",
    "\n",
    "\n",
    "    #20230817_113746_1.2_FCN\n",
    "    FCN_1_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230817_113746_1_merged.rec\\\\20230817_113746_1.2_2t2bL_FCN.xlsx\")\n",
    "    times_FCN_1_2 = [599, 604, 1200, 1203, 1499, 1506, 1811, 1817]\n",
    "    order_FCN_1_2 = ['familiar', 'cagemate', 'novel']\n",
    "    FCN_1_2_media_duration = 2110.67 * 1000\n",
    "    FCN_1_2_last_timestamp = FCN_1_2.timestamps_var[-1]\n",
    "\n",
    "\n",
    "\n",
    "    #20230803_141047_1.2_CNF\n",
    "    CNF_1_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230803_141047_1_merged.rec\\\\20230803 _ 141047 _1.2 2t2bL CNF .xlsx\")\n",
    "    times_CNF_1_2 = [600, 605, 1200, 1203, 1500, 1506, 1800, 1806]\n",
    "    order_CNF_1_2 = ['cagemate', 'novel', 'familiar']\n",
    "    CNF_1_2_media_duration = 2101.170 * 1000\n",
    "    CNF_1_2_last_timestamp = CNF_1_2.timestamps_var[-1]\n",
    "\n",
    "\n",
    "    #20230804_121600_1.4_FCN\n",
    "    FCN_1_4_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230804_121600_1_merged.rec\\\\20230804_121600_1.4_3t3bL_FCN.xlsx\")\n",
    "    times_FCN_1_4 = [600, 607, 1203, 1206, 1499, 1505, 1799, 1807]\n",
    "    order_FCN_1_4 = ['familiar', 'cagemate', 'novel']\n",
    "    FCN_1_4_media_duration = 2101.00 * 1000\n",
    "    FCN_1_4_last_timestamp = FCN_1_4.timestamps_var[-1]\n",
    "\n",
    "\n",
    "    #20230818_133620_1.4_CNF\n",
    "    CNF_1_4_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230818_133620_1_merged.rec\\\\20230818_133620_1.4_3t3bL_CNF.xlsx\")\n",
    "    times_CNF_1_4 = [599, 605, 1158, 1202, 1497, 1503, 1800, 1806]\n",
    "    order_CNF_1_4 = ['cagemate', 'novel', 'familiar']\n",
    "    CNF_1_4_media_duration = 2100.333 * 1000\n",
    "    CNF_1_4_last_timestamp = CNF_1_4.timestamps_var[-1]\n",
    "\n",
    "    #20230803_121318_1.4_NFC\n",
    "    NFC_1_4_1_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230803_121318_1_merged.rec\\\\20230803 121318 1.1.xlsx\")\n",
    "    NFC_1_4_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230803_121318_1_merged.rec\\\\20230803 121318 1.2.xlsx\")\n",
    "    times_NFC_1_4_1 = [599, 604, 1021, 0, 0, 0, 0, 99999999]\n",
    "    times_NFC_1_4_2 =  [0, 0, 50, 53, 347, 354, 650, 656]\n",
    "    order_NFC_1_4 = ['novel', 'familiar', 'cagemate']\n",
    "    NFC_1_4_media_duration = 951.633 * 1000\n",
    "    NFC_1_4_last_timestamp = NFC_1_4.timestamps_var[-1]\n",
    "    NFC_1_4_media_duration1 = 1021.333 * 1000\n",
    " \n",
    "    len_events = 10\n",
    "    no_events = 10\n",
    "    random_event_dict = {}\n",
    "\n",
    "    CNF_1_1_arrays = p2_create_random_array(times_CNF_1_1, order_CNF_1_1, len_events, no_events, CNF_1_1_media_duration)\n",
    "    random_event_dict['20230817_100823_1_merged.rec'] = CNF_1_1_arrays\n",
    "\n",
    "    FCN_1_1_arrays = p2_camera_crash_random(times_FCN_1_1_1, times_FCN_1_1_2, order_FCN_1_1, FCN_1_1_media_duration,\n",
    "                                            FCN_1_1_last_timestamp, len_events, no_events)\n",
    "    random_event_dict['20230803_101331_1_merged.rec'] = FCN_1_1_arrays\n",
    "\n",
    "    NFC_1_1_arrays = p2_camera_crash_random(times_NFC_1_1_1, times_NFC_1_1_2, order_NFC_1_1, NFC_1_1_media_duration,\n",
    "                                            NFC_1_1_last_timestamp, len_events, no_events)\n",
    "    random_event_dict['20230818_115728_1_merged.rec'] = NFC_1_1_arrays\n",
    "\n",
    "    NFC_1_2_arrays = p2_create_random_array(times_NFC_1_2, order_NFC_1_2, len_events, no_events, NFC_1_2_media_duration)\n",
    "    random_event_dict['20230804_141009_1_merged.rec'] = NFC_1_2_arrays\n",
    "\n",
    "    FCN_1_2_arrays = p2_create_random_array(times_FCN_1_2, order_FCN_1_2, len_events, no_events, FCN_1_2_media_duration)\n",
    "    random_event_dict['20230817_113746_1_merged.rec'] = FCN_1_2_arrays\n",
    "\n",
    "    CNF_1_2_arrays = p2_create_random_array(times_CNF_1_2, order_CNF_1_2, len_events, no_events, CNF_1_2_media_duration)\n",
    "    random_event_dict['20230803_141047_1_merged.rec'] = CNF_1_2_arrays\n",
    "\n",
    "    FCN_1_4_arrays = p2_create_random_array(times_FCN_1_4, order_FCN_1_4, len_events, no_events, FCN_1_4_media_duration)\n",
    "    random_event_dict['20230804_121600_1_merged.rec'] = FCN_1_4_arrays\n",
    "\n",
    "    CNF_1_4_arrays = p2_create_random_array(times_CNF_1_4, order_CNF_1_4, len_events, no_events, CNF_1_4_media_duration)\n",
    "    random_event_dict['20230818_133620_1_merged.rec'] = CNF_1_4_arrays\n",
    "\n",
    "    NFC_1_4_arrays = p2_camera_crash_random(times_NFC_1_4_1, times_NFC_1_4_2, order_NFC_1_4, NFC_1_4_media_duration,\n",
    "                                            NFC_1_4_last_timestamp, len_events, no_events)\n",
    "    random_event_dict['20230803_121318_1_merged.rec'] = NFC_1_4_arrays\n",
    "except FileNotFoundError:\n",
    "    print('File not found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_event_dict = unpickle_this('random_event_dict.pkl')\n",
    "assign_dicts(phase2_collection, random_event_dict)\n",
    "randomsnippet = spike.SpikeAnalysis_MultiRecording(phase2_collection, 250, 100, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {'cagemate': 'k', 'acquisition': 'b', 'recall': 'r', 'novel': 'g'}\n",
    "recording_number = 0\n",
    "plt.figure(figsize = (20,12))\n",
    "for name, recording in phase2_collection.collection.items():\n",
    "    for event, snippets in recording.event_dict.items():\n",
    "        if (event =='cagemate') | (event == 'recall') | (event == 'novel') | (event == 'acquisition'):\n",
    "            for i in range(len(snippets)):\n",
    "                y = [recording_number,recording_number]\n",
    "                x = snippets[i]/1000/60\n",
    "            # Plot the line segments\n",
    "                plt.plot(x, y, marker='o', linestyle='-', c= color_dict[event])\n",
    "    recording_number += 1\n",
    "# Optionally, you can add labels, title, etc.\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Line Segments')\n",
    "legend_labels = [plt.Line2D([0], [0], color=color_dict[label], lw=2, label=label) for label in color_dict]\n",
    "plt.legend(handles=legend_labels)\n",
    "plt.grid(True)\n",
    "plt.xlim([0,35])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {'cagemate': 'k', 'acquisition': 'b', 'recall': 'r', 'novel': 'g'}\n",
    "recording_number = 0\n",
    "for name, recording in phase2_collection.collection.items():\n",
    "    for event, snippets in recording.event_dict.items():\n",
    "        if (event =='cagemate') | (event == 'recall') | (event == 'novel') | (event == 'acquisition'):\n",
    "            for i in range(len(snippets)):\n",
    "                y = [recording_number,recording_number]\n",
    "                x = snippets[i]/1000/60\n",
    "            # Plot the line segments\n",
    "                plt.plot(x, y, marker='o', linestyle='-', c= color_dict[event])\n",
    "    recording_number += 1\n",
    "# Optionally, you can add labels, title, etc.\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Line Segments')\n",
    "legend_labels = [plt.Line2D([0], [0], color=color_dict[label], lw=2, label=label) for label in color_dict]\n",
    "plt.legend(handles=legend_labels)\n",
    "plt.grid(True)\n",
    "plt.xlim([0,35])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {'exposure 0': 'k', 'exposure 1': 'b', 'exposure 2': 'r', 'exposure 3': 'g'}\n",
    "\n",
    "recording_no = 0\n",
    "for name, recording in phase2_collection.collection.items():\n",
    "    for event, snippets in recording.event_dict.items():\n",
    "        if (event =='exposure 0') | (event == 'exposure 1') | (event == 'exposure 2') | (event == 'exposure 3'):\n",
    "            for i in range(len(snippets)):\n",
    "                y = [recording_no, recording_no]\n",
    "                x = snippets[i]/1000/60\n",
    "            # Plot the line segments\n",
    "                plt.plot(x, y, marker='o', linestyle='-', c= color_dict[event])\n",
    "    recording_no += 1\n",
    "\n",
    "# Optionally, you can add labels, title, etc.\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Line Segments')\n",
    "legend_labels = [plt.Line2D([0], [0], color=color_dict[label], lw=2, label=label) for label in color_dict]\n",
    "plt.legend(handles=legend_labels)\n",
    "plt.grid(True)\n",
    "plt.xlim([0,35])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ran_snip_auc_id, ran_snip_prob_id] = randomsnippet.trial_decoder(equalize = 10,\n",
    "                                                 pre_window = 0,\n",
    "                                                 post_window = 0,\n",
    "                                                 num_fold=5,\n",
    "                                                 num_shuffle=5,\n",
    "                                                 no_PCs= 8,\n",
    "                                                 events = ['novel', 'cagemate', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ran_snip_auc, ran_snip_prob] = randomsnippet.trial_decoder(equalize = 10,\n",
    "                                                 pre_window = 0,\n",
    "                                                 post_window = 0,\n",
    "                                                 num_fold=5,\n",
    "                                                 num_shuffle=5,\n",
    "                                                 no_PCs= 8,\n",
    "                                                 events = ['exposure 1', 'exposure 2', 'exposure 3'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
