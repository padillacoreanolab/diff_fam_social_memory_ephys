{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\megha\\anaconda3\\envs\\ephys_analysis\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pathlib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trodes_extract_single_file(trodes_directory, file, mode=\"-raw\"):\n",
    "    \"\"\"\n",
    "    This automates Trodes extraction by running a sequence of commands through\n",
    "    the terminal. Trodes application is needed to be installed into the computer\n",
    "    to run this function. Extraction folders will be generated within each .rec\n",
    "    data folder per recording.\n",
    "    Progress reports are printed: 'x out of n files extracted' after each extraction\n",
    "\n",
    "    -raw file is needed to obtain timestamps from the ephys recording file.\n",
    "    -dio and -analogio are needed for ECU time stamps\n",
    "\n",
    "    Args(3 total, 2 required):\n",
    "        trodes_directory: str, directory to the trodes application folder\n",
    "        data: str, directory to your Trodes data folder\n",
    "              all files within data folder should be the .rec folders autogenerated\n",
    "              by trodes extractions\n",
    "        mode: str, default='-raw', a Trodes setting specifying which types\n",
    "              of files you want extracted. must be written as '-mode -mode'\n",
    "              for each mode desired.\n",
    "            Modes as described by Trodes:\n",
    "                -spikes: Spike waveform export (spike snippets only).\n",
    "\n",
    "                -lfp: Continuous LFP band export.\n",
    "\n",
    "                -spikeband: Continuous spike band export.\n",
    "\n",
    "                -raw: Continuous raw band export.\n",
    "\n",
    "                -stim: Continuous stimulation band export for stimulation capable recording channels.\n",
    "\n",
    "                -dio: Digital IO channel state change export.\n",
    "\n",
    "                -analogio: Continuous analog IO export.\n",
    "\n",
    "                -mountainsort: One mountainsort file per ntrode (using raw band data)\n",
    "\n",
    "                -kilosort: One kilosort file per ntrode (using raw band data).\n",
    "\n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    command = \"trodesexport \" + mode + \" -rec \" + file\n",
    "    subprocess.Popen(command, shell=True, cwd=trodes_directory).wait()\n",
    "    print(\"files successfully extracted\")\n",
    "\n",
    "def trodes_extract(trodes_directory, data, mode='-raw'):\n",
    "    \"\"\"\n",
    "    This automates Trodes extraction by running a sequence of commands through\n",
    "    the terminal. Trodes application is needed to be installed into the computer\n",
    "    to run this function. Extraction folders will be generated within each .rec \n",
    "    data folder per recording. \n",
    "    Progress reports are printed: 'x out of n files extracted' after each extraction\n",
    "\n",
    "    -raw file is needed to obtain timestamps from the ephys recording file. \n",
    "    -dio and -analogio are needed for ECU time stamps\n",
    "\n",
    "    Args(3 total, 2 required):\n",
    "        trodes_directory: str, directory to the trodes application folder \n",
    "        data: str, directory to your Trodes data folder \n",
    "              all files within data folder should be the .rec folders autogenerated\n",
    "              by trodes extractions\n",
    "        mode: str, default='-raw', a Trodes setting specifying which types \n",
    "              of files you want extracted. must be written as '-mode -mode'\n",
    "              for each mode desired.\n",
    "            Modes as described by Trodes: \n",
    "                -spikes: Spike waveform export (spike snippets only).\n",
    "\n",
    "                -lfp: Continuous LFP band export.\n",
    "\n",
    "                -spikeband: Continuous spike band export.\n",
    "\n",
    "                -raw: Continuous raw band export.\n",
    "\n",
    "                -stim: Continuous stimulation band export for stimulation capable recording channels.\n",
    "\n",
    "                -dio: Digital IO channel state change export.\n",
    "\n",
    "                -analogio: Continuous analog IO export.\n",
    "\n",
    "                -mountainsort: One mountainsort file per ntrode (using raw band data)\n",
    "\n",
    "                -kilosort: One kilosort file per ntrode (using raw band data).\n",
    "    \n",
    "    Returns: \n",
    "        none\n",
    "\"\"\"\n",
    "    data_path = data + '\\*'\n",
    "    folders = glob.glob(data_path)\n",
    "    files = [glob.glob(f'{folder}\\*merged.rec') for folder in folders]\n",
    "    commands = []\n",
    "    for i, folder in enumerate(folders):\n",
    "        if files[i]:\n",
    "            for file in files[i]:\n",
    "                command =  'trodesexport '+ mode + ' -rec '+ file\n",
    "                commands.append(command)\n",
    "    no_commands = len(commands)\n",
    "    i = 0\n",
    "    for command in commands:\n",
    "        subprocess.Popen(command, shell=True, cwd=trodes_directory).wait()\n",
    "        i +=1\n",
    "        print(i, 'out of', len(commands), 'files extracted')\n",
    "\n",
    "def parse_fields(field_str):\n",
    "    \"\"\"\n",
    "    Parses a string of fields into a numpy data type object.\n",
    "\n",
    "    The input string should be formatted as '<fieldname num*type>' or \n",
    "    '<fieldname type>'. This function parses the string, extracts the field \n",
    "    names and data types, and creates a numpy data type object which can be \n",
    "    used to read data from a binary file.\n",
    "\n",
    "    Args:\n",
    "        field_str (str): The string specifying the fields.\n",
    "\n",
    "    Returns:\n",
    "        np.dtype: A numpy data type object that describes the structure of \n",
    "                  the data.\n",
    "\n",
    "    Raises:\n",
    "        SystemExit: If the provided field type is not a valid numpy data type.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean up the string and split it into components\n",
    "    components = re.split('\\s', re.sub(r\"\\>\\<|\\>|\\<\", ' ', field_str).strip())\n",
    "    \n",
    "    dtype_spec = []  # Will hold tuples to specify the numpy data type\n",
    "    \n",
    "    # Iterate over pairs of components (field name and type)\n",
    "    for i in range(0, len(components), 2):\n",
    "        field_name = components[i]\n",
    "        \n",
    "        # Default values\n",
    "        repeat_count = 1\n",
    "        field_type_str = 'uint32'\n",
    "        \n",
    "        # If the field type string contains a '*', it indicates a repeat count\n",
    "        if '*' in components[i+1]:\n",
    "            split_types = re.split('\\*', components[i+1])\n",
    "            # Handle both 'num*type' and 'type*num'\n",
    "            field_type_str = split_types[split_types[0].isdigit()]\n",
    "            repeat_count = int(split_types[split_types[1].isdigit()])\n",
    "        else:\n",
    "            field_type_str = components[i+1]\n",
    "        \n",
    "        # Convert the field type string to an actual numpy data type\n",
    "        try:\n",
    "            field_type = getattr(np, field_type_str)\n",
    "        except AttributeError:\n",
    "            print(f\"{field_type_str} is not a valid field type.\")\n",
    "            exit(1)\n",
    "        else:\n",
    "            dtype_spec.append((str(field_name), field_type, repeat_count))\n",
    "    \n",
    "    return np.dtype(dtype_spec)\n",
    "\n",
    "def read_trodes_extracted_data_file(filename):\n",
    "    \"\"\"\n",
    "    Reads the content of a Trodes extracted data file.\n",
    "\n",
    "    This function opens a Trodes file, reads the settings, parses them into a dictionary, \n",
    "    and then reads the remaining data in the file as a numpy array according to the \n",
    "    data types specified in the settings. If the settings block does not start correctly,\n",
    "    it raises an Exception.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the Trodes file to be read.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are settings field names and values are the \n",
    "              corresponding setting values. The actual data from the file is stored \n",
    "              under the 'data' key as a numpy array.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If the settings block in the file does not start with '<Start settings>'.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        # The first line of the file should start the settings block\n",
    "        if f.readline().decode('ascii').strip() != '<Start settings>':\n",
    "            raise Exception(\"Settings format not supported\")\n",
    "        \n",
    "        # Flag indicating we're reading the settings block\n",
    "        fields = True\n",
    "        # Dictionary to hold the settings fields and values\n",
    "        fields_text = {}\n",
    "        \n",
    "        # Iterate over the lines in the file\n",
    "        for line in f:\n",
    "            # If we're still reading the settings block\n",
    "            if fields:\n",
    "                line = line.decode('ascii').strip()\n",
    "                # If we've not reached the end of the settings block, continue reading fields\n",
    "                if line != '<End settings>':\n",
    "                    key, value = line.split(': ')\n",
    "                    fields_text.update({key.lower(): value})\n",
    "                # If we've reached the end of the settings block, stop reading fields\n",
    "                else:\n",
    "                    fields = False\n",
    "                    # Parse the 'fields' setting to get the data type\n",
    "                    dt = parse_fields(fields_text['fields'])\n",
    "                    fields_text['data'] = np.zeros([1], dtype = dt)\n",
    "                    break\n",
    "        \n",
    "        # Read the remaining data from the file using the parsed data type\n",
    "        dt = parse_fields(fields_text['fields'])\n",
    "        data = np.fromfile(f, dt)\n",
    "        fields_text.update({'data': data})\n",
    "        return fields_text\n",
    "            \n",
    "def organize_single_trodes_export(dir_path, skip_raw_group0=True):\n",
    "    \"\"\"\n",
    "    Organizes Trodes data files in a given directory. The data is stored in a dictionary. \n",
    "    The key is the penultimate (second to last) part of the file name (i.e., the part before the last dot in the file name). \n",
    "    The values in the dictionary are the parsed data from the Trodes files.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): The path to the directory containing the Trodes files.\n",
    "        skip_raw_group0(bool): To skip the \"raw_group0\" file which contains the raw signal which uses a lot of memory\n",
    "    Returns:\n",
    "        dict: A dictionary with organized Trodes file data.\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to store results\n",
    "    result = {}\n",
    "    \n",
    "    # Iterate over all files in the directory\n",
    "    for file_name in os.listdir(dir_path):\n",
    "\n",
    "        if skip_raw_group0 and \"raw_group0\" in file_name:\n",
    "            continue\n",
    "        # Attempt to parse each file and store the data in the dictionary\n",
    "        try:\n",
    "            # Extract second to last part of the file name\n",
    "            sub_dir_name = file_name.rsplit('.', 2)[-2]\n",
    "            # Parse Trodes file and store the data\n",
    "            result[sub_dir_name] = read_trodes_extracted_data_file(os.path.join(dir_path, file_name))\n",
    "\n",
    "        # Skip files that cause errors during parsing\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {file_name} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    return result\n",
    "\n",
    "def organize_all_trodes_export(dir_path):\n",
    "    \"\"\"\n",
    "    Organize Trodes files in subdirectories based on prefix and suffix of the subdirectory.\n",
    "    The function creates a dictionary with subdirectory prefix and suffix as keys,\n",
    "    and the output of `organize_trodes_files_by_suffix` as values.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): Path of the directory to process.\n",
    "\n",
    "    Returns:\n",
    "        dict: Nested dictionary with keys as subdirectory prefix and suffix and values \n",
    "        containing data obtained from the `organize_trodes_files_by_suffix` function.\n",
    "    \"\"\"\n",
    "    result = defaultdict(dict)\n",
    "    \n",
    "    for sub_dir_name in os.listdir(dir_path):\n",
    "        # Construct the full path to the subdirectory\n",
    "        sub_dir_path = os.path.join(dir_path, sub_dir_name)\n",
    "        # Process only if it's a directory\n",
    "        if os.path.isdir(sub_dir_path):\n",
    "            try:\n",
    "                # Split the subdirectory name by dots to extract prefix and suffix\n",
    "                sub_dir_name_parts = sub_dir_name.split('.')\n",
    "                sub_dir_name_prefix = sub_dir_name_parts[0]\n",
    "                sub_dir_name_suffix = sub_dir_name_parts[-1]\n",
    "                # Organize the Trodes files in the subdirectory and store the results\n",
    "                result[sub_dir_name_prefix][sub_dir_name_suffix] = organize_single_trodes_export(sub_dir_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing subdirectory {sub_dir_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return result\n",
    "def create_timestamps_dict(data_directory):\n",
    "    \"\"\"\n",
    "    creates a dictionary of timestamps for each recording in the given directory \n",
    "    timestamps are zeroed to the start of the stream. timestamps are the sample number\n",
    "    or sample index \n",
    "    two main elements of the dictionary are\n",
    "    1. ephys recording timestamps\n",
    "    2. video timestamps: another dictionary whose keys are the video names\n",
    "       and values are timestamps for each frame of the video, i.e. the sample number\n",
    "       for each frame of the video \n",
    "\n",
    "    Args(1):\n",
    "        data_directory: str, path to data file \n",
    "            \\\\with\\\\doube\\\\slashes\n",
    "    \n",
    "    Returns (1):\n",
    "        session_timestamps: dict, recordings to timestamps\n",
    "            key: str, file name minus _merged.rec\n",
    "            value: dict whose keys and values are\n",
    "                'vid timestamps': {name of video, str: vid timestamps, np.array}\n",
    "                'ephys timestamps': np.array of ephys sample timestamps\n",
    "\n",
    "    \"\"\"\n",
    "    input_glob = data_directory + '\\*.rec'\n",
    "    all_session_files = glob.glob(input_glob)\n",
    "    session_timestamps = {}\n",
    "    for session_path in all_session_files:   \n",
    "        vid_dict = {}\n",
    "        session_basename = os.path.splitext(os.path.basename(session_path))[0]\n",
    "        first_timestamp = None\n",
    "        for root, dirs, files in os.walk(session_path):\n",
    "            if os.path.basename(root).endswith('merged.raw'):\n",
    "                for file in files:\n",
    "                    if file.endswith('timestamps.dat'):\n",
    "                        timestamps = os.path.join(root, file)\n",
    "                        ephys_timestamp_array = read_trodes_extracted_data_file(timestamps)\n",
    "                        ephys_timestamp = ephys_timestamp_array['data']\n",
    "                        first_timestamp = ephys_timestamp_array['first_timestamp']  \n",
    "            for file in files:\n",
    "                if file.endswith('cameraHWSync'):\n",
    "                    vid_timestamps_path = os.path.join(session_path, file)\n",
    "                    timestamp_array = read_trodes_extracted_data_file(vid_timestamps_path)\n",
    "                    vid_timestamps = timestamp_array[\"data\"][\"PosTimestamp\"]\n",
    "                    vid_dict[file] = vid_timestamps\n",
    "        session_timestamps[session_basename] = {'vid timestamps': vid_dict, \n",
    "                                                'ephys timestamps':ephys_timestamp}\n",
    "    \n",
    "    return session_timestamps, first_timestamp\n",
    "\n",
    "def find_closest_index(sorted_list=None, target=0):\n",
    "    #leo's function that i am not sure i want to use at the moment \n",
    "    \"\"\"\n",
    "    Returns the index of the number in the sorted list that is closest to the target.\n",
    "\n",
    "    This function performs a binary search on a sorted list to find the closest number to \n",
    "    a given target. If the target exists in the list, its index is returned. If not, the \n",
    "    function will return the index of the number that's closest to the target.\n",
    "\n",
    "    Parameters:\n",
    "    - sorted_list (list[int or float]): A sorted list of numbers.\n",
    "    - target (int or float): The target number to find the closest value to.\n",
    "\n",
    "    Returns:\n",
    "    - int: The index of the closest number in the sorted list to the target. \n",
    "           If the sorted list is empty, returns None.\n",
    "\n",
    "    Example:\n",
    "    >>> sorted_nums = [1, 3, 5, 8, 10, 15, 18, 20, 24, 27, 30]\n",
    "    >>> find_closest_index(sorted_nums, 6)\n",
    "    2\n",
    "\n",
    "    Note:\n",
    "    The list should be sorted in ascending order.\n",
    "    \"\"\"\n",
    "    \n",
    "    if sorted_list is None:\n",
    "        return None\n",
    "    if target <= sorted_list[0]:\n",
    "        return 0\n",
    "    if target >= sorted_list[-1]:\n",
    "        return len(sorted_list) - 1\n",
    "\n",
    "    # Binary search\n",
    "    left, right = 0, len(sorted_list) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "\n",
    "        if sorted_list[mid] == target:\n",
    "            return mid\n",
    "        elif sorted_list[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "\n",
    "    # After binary search, the target will be between sorted_list[right] and sorted_list[left]\n",
    "    # We compare the two to see which one is closer to the target and return its index\n",
    "    if abs(sorted_list[left] - target) < abs(sorted_list[right] - target):\n",
    "        return left\n",
    "    else:\n",
    "        return right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 1 files extracted\n"
     ]
    }
   ],
   "source": [
    "trodes_directory = r\"C:\\Users\\megha\\Downloads\\Trodes_2-5-2_Windows64\\Trodes_2-5-2_Windows64\"\n",
    "data = r\"D:\\cups\\test\"\n",
    "trodes_extract(trodes_directory, data, mode='-time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megha\\AppData\\Local\\Temp\\ipykernel_37160\\2057214871.py:109: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  return np.dtype(dtype_spec)\n"
     ]
    }
   ],
   "source": [
    "timestamps = read_trodes_extracted_data_file(r\"D:\\cups\\test\\11_cups_p4.rec\\11_cups_p4_merged.time\\11_cups_p4_merged.timestamps.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_timestamp = timestamps['data']\n",
    "first_timestamp = timestamps['first_timestamp']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1675517\n"
     ]
    }
   ],
   "source": [
    "print(first_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megha\\AppData\\Local\\Temp\\ipykernel_37160\\2057214871.py:109: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  return np.dtype(dtype_spec)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ephys_timestamp' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m session_timestamps, first_timestamp \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_timestamps_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 276\u001b[0m, in \u001b[0;36mcreate_timestamps_dict\u001b[1;34m(data_directory)\u001b[0m\n\u001b[0;32m    273\u001b[0m                 vid_timestamps \u001b[38;5;241m=\u001b[39m timestamp_array[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPosTimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    274\u001b[0m                 vid_dict[file] \u001b[38;5;241m=\u001b[39m vid_timestamps\n\u001b[0;32m    275\u001b[0m     session_timestamps[session_basename] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvid timestamps\u001b[39m\u001b[38;5;124m'\u001b[39m: vid_dict, \n\u001b[1;32m--> 276\u001b[0m                                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mephys timestamps\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mephys_timestamp\u001b[49m}\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session_timestamps, first_timestamp\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'ephys_timestamp' referenced before assignment"
     ]
    }
   ],
   "source": [
    "session_timestamps, first_timestamp = create_timestamps_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files successfully extracted\n"
     ]
    }
   ],
   "source": [
    "file = r\"D:\\cups\\data\\12_cups_p4.rec\\12_cups_p4_merged.rec\"\n",
    "trodes_extract_single_file(trodes_directory=trodes_directory, file=file, mode = '-time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\megha\\AppData\\Local\\Temp\\ipykernel_37160\\130899744.py:154: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  return np.dtype(dtype_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823805\n"
     ]
    }
   ],
   "source": [
    "\n",
    "timestamps = read_trodes_extracted_data_file(r\"D:\\cups\\data\\12_cups_p4.rec\\12_cups_p4_merged.time\\12_cups_p4_merged.timestamps.dat\")\n",
    "print(timestamps['first_timestamp'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\cups\\data\\12_cups_p4.rec\\12_cups_p4_merged.time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "timestamps_path = str(Path(file).with_suffix('.time'))\n",
    "timestamps_path = os.path.normpath(timestamps_path)\n",
    "print(timestamps_path)\n",
    "os.path.exists(timestamps_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_file_path = os.path.join(timestamps_path, '*.timestamps.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\cups\\data\\12_cups_p4.rec\\12_cups_p4_merged.time\\12_cups_p4_merged.timestamps.dat\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(timestamps_path):\n",
    "    if file.endswith('.timestamps.dat'):\n",
    "        print(os.path.join(timestamps_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(r\"D:\\cups\\data\\12_cups_p4.rec\\12_cups_p4_merged.time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\cups\\\\data\\\\12_cups_p4.time\\\\12_cups_p4_merged.time\\\\*.timestamps.dat'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps_file_path = os.path.join(timestamps_path, '*.timestamps.dat')\n",
    "timestamps_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_recording = r\"C:\\Users\\megha\\Documents\\GitHub\\diff_fam_social_memory_ephys\\lfp\\tests\\test_data\\Example_Recording\\example_recording.rec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files successfully extracted\n"
     ]
    }
   ],
   "source": [
    "trodes_extract_single_file(trodes_directory=trodes_directory, file=example_recording, mode = '-time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
