{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import os\n",
    "from scipy.stats import sem\n",
    "from matplotlib.lines import Line2D\n",
    "import sys\n",
    "sys.path.append(\"../../spike\")\n",
    "import multirecording_spikeanalysis as spike\n",
    "import pickle\n",
    "\n",
    "def hex_2_rgb(hex_color): # Orange color\n",
    "    rgb_color = tuple(int(hex_color[i:i+2], 16) / 255.0 for i in (1, 3, 5))\n",
    "    return rgb_color\n",
    "\n",
    "def pickle_this(thing_to_pickle, file_name):\n",
    "    \"\"\"\n",
    "    Pickles things\n",
    "    Args (2):   \n",
    "        thing_to_pickle: anything you want to pickle\n",
    "        file_name: str, filename that ends with .pkl \n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    with open(file_name,'wb') as file:\n",
    "        pickle.dump(thing_to_pickle, file)\n",
    "\n",
    "def unpickle_this(pickle_file):\n",
    "    \"\"\"\n",
    "    Unpickles things\n",
    "    Args (1):   \n",
    "        file_name: str, pickle filename that already exists and ends with .pkl\n",
    "    Returns:\n",
    "        pickled item\n",
    "    \"\"\"\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        return(pickle.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_dicts = unpickle_this('behavior_dicts.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  11_cage_p1_merged.rec\n",
      "loading  12_cage_p1_merged.rec\n",
      "loading  13_cage_p1_merged.rec\n",
      "loading  21_cage_p1_merged.rec\n",
      "loading  22_cage_p1_merged.rec\n",
      "loading  23_cage_p1_merged.rec\n",
      "loading  31_cage_p1_merged.rec\n",
      "loading  32_cage_p1_merged.rec\n",
      "loading  33_cage_p1_merged.rec\n",
      "loading  41_cage_p1_merged.rec\n",
      "loading  44_cage_p1_merged.rec\n",
      "Please assign event dictionaries to each recording\n",
      "as recording.event_dict\n",
      "event_dict = {event name(str): np.array[[start(ms), stop(ms)]...]\n",
      "Please assign subjects to each recording as recording.subject\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cagemate_collection = spike.EphysRecordingCollection(r\"C:\\Users\\megha\\UFL Dropbox\\Meghan Cum\\Padilla-Coreano Lab\\2024\\Cum_SocialMemEphys_pilot2\\Habituation_Dishabituation (phase 1)\\sorted\\cagemate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading  11_nov_p1_merged.rec\n",
      "loading  12_nov_p1_merged.rec\n",
      "loading  13_nov_p1_merged.rec\n",
      "loading  21_nov_p1_merged.rec\n",
      "loading  22_nov_p1_merged.rec\n",
      "loading  23_nov_p1_merged.rec\n",
      "loading  24_nov_p1_merged.rec\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\megha\\\\UFL Dropbox\\\\Meghan Cum\\\\Padilla-Coreano Lab\\\\2024\\\\Cum_SocialMemEphys_pilot2\\\\Habituation_Dishabituation (phase 1)\\\\sorted\\\\novel\\\\24_nov_p1_merged.rec\\\\phy\\\\cluster_group.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m novel_collection \u001b[38;5;241m=\u001b[39m \u001b[43mspike\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEphysRecordingCollection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmegha\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUFL Dropbox\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMeghan Cum\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mPadilla-Coreano Lab\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m2024\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCum_SocialMemEphys_pilot2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mHabituation_Dishabituation (phase 1)\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msorted\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mnovel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\megha\\Documents\\GitHub\\diff_fam_social_memory_ephys\\pilot2\\habit_dishabit_phase1\\../../spike\\multirecording_spikeanalysis.py:477\u001b[0m, in \u001b[0;36mEphysRecordingCollection.__init__\u001b[1;34m(self, path, sampling_rate)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPCA_dfs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfishers_exact \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 477\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease assign event dictionaries to each recording\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas recording.event_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\megha\\Documents\\GitHub\\diff_fam_social_memory_ephys\\pilot2\\habit_dishabit_phase1\\../../spike\\multirecording_spikeanalysis.py:489\u001b[0m, in \u001b[0;36mEphysRecordingCollection.make_collection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m directory\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged.rec\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    488\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;124m\"\u001b[39m, directory)\n\u001b[1;32m--> 489\u001b[0m             tempobject \u001b[38;5;241m=\u001b[39m \u001b[43mEphysRecording\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m                \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m             collection[directory] \u001b[38;5;241m=\u001b[39m tempobject\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection \u001b[38;5;241m=\u001b[39m collection\n",
      "File \u001b[1;32mc:\\Users\\megha\\Documents\\GitHub\\diff_fam_social_memory_ephys\\pilot2\\habit_dishabit_phase1\\../../spike\\multirecording_spikeanalysis.py:366\u001b[0m, in \u001b[0;36mEphysRecording.__init__\u001b[1;34m(self, path, sampling_rate)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzscored_events \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwilcox_dfs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 366\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_unit_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_spike_specs()\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unit_timestamps()\n",
      "File \u001b[1;32mc:\\Users\\megha\\Documents\\GitHub\\diff_fam_social_memory_ephys\\pilot2\\habit_dishabit_phase1\\../../spike\\multirecording_spikeanalysis.py:383\u001b[0m, in \u001b[0;36mEphysRecording.get_unit_labels\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03massigns self.labels_dicts as a dictionary\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03mwith unit id (str) as key and label as values (str)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_group.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    384\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(f, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_dict \u001b[38;5;241m=\u001b[39m {row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader}\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\megha\\\\UFL Dropbox\\\\Meghan Cum\\\\Padilla-Coreano Lab\\\\2024\\\\Cum_SocialMemEphys_pilot2\\\\Habituation_Dishabituation (phase 1)\\\\sorted\\\\novel\\\\24_nov_p1_merged.rec\\\\phy\\\\cluster_group.tsv'"
     ]
    }
   ],
   "source": [
    "novel_collection = spike.EphysRecordingCollection(r\"C:\\Users\\megha\\UFL Dropbox\\Meghan Cum\\Padilla-Coreano Lab\\2024\\Cum_SocialMemEphys_pilot2\\Habituation_Dishabituation (phase 1)\\sorted\\novel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
