{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boris_extraction as boris\n",
    "import multirecording_spikeanalysis as spike\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel\n",
    "import matplotlib.patches as mpatches\n",
    "from itertools import combinations\n",
    "\n",
    "def hex_2_rgb(hex_color): # Orange color\n",
    "    rgb_color = tuple(int(hex_color[i:i+2], 16) / 255.0 for i in (1, 3, 5))\n",
    "    return rgb_color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_this(thing_to_pickle, file_name):\n",
    "    \"\"\"\n",
    "    Pickles things\n",
    "    Args (2):   \n",
    "        thing_to_pickle: anything you want to pickle\n",
    "        file_name: str, filename that ends with .pkl \n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    with open(file_name,'wb') as file:\n",
    "        pickle.dump(thing_to_pickle, file)\n",
    "\n",
    "def unpickle_this(pickle_file):\n",
    "    \"\"\"\n",
    "    Unpickles things\n",
    "    Args (1):   \n",
    "        file_name: str, pickle filename that already exists and ends with .pkl\n",
    "    Returns:\n",
    "        pickled item\n",
    "    \"\"\"\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        return(pickle.load(file))\n",
    "\n",
    "\n",
    "def random_event_generator(start, stop, len_event, no_events):\n",
    "    \"\"\"\n",
    "    Takes in start and stop times, len_event in seconds\n",
    "    Returns a numpy array of [no_events, 2] in ms\n",
    "    If start and stop are too short to do the request it will return [[0,0]]\n",
    "    \"\"\"\n",
    "    total_duration = stop - start\n",
    "    possible_events = np.arange(int(total_duration / len_event))\n",
    "    try:\n",
    "        pot_events = np.random.choice(possible_events, size = (no_events), replace = False)\n",
    "        pot_events = np.sort(pot_events)\n",
    "        events = []\n",
    "        for i in pot_events: \n",
    "            event_start = (start + (len_event * i)) * 1000\n",
    "            event_stop = (event_start + (len_event * 1000))\n",
    "            events.append(np.array([event_start, event_stop]))\n",
    "        return(np.array(events))\n",
    "    except ValueError:\n",
    "        print('camera crash')\n",
    "        return(np.zeros([1,2]))\n",
    "    \n",
    "\n",
    "def p2_create_random_array(times, order, len_event, no_events, media_duration):\n",
    "    media_duration = media_duration/1000\n",
    "    acquisition_array = random_event_generator(0, times[0], len_event, no_events)\n",
    "    exposure1_array = random_event_generator(times[3], times[4], len_event, no_events)\n",
    "    exposure2_array = random_event_generator(times[5], times[6], len_event, no_events)\n",
    "    exposure3_array = random_event_generator(times[7], media_duration, len_event, no_events)\n",
    "    order_arrays = [exposure1_array, exposure2_array, exposure3_array]\n",
    "    for i in range(len(order)):\n",
    "        if order[i] == 'familiar':\n",
    "            recall_array = order_arrays[i]\n",
    "        if order[i] == 'cagemate':\n",
    "            cagemate_array = order_arrays[i]\n",
    "        if order[i] == 'novel':\n",
    "            novel_array = order_arrays[i]\n",
    "    event_dict = {'acquisition': acquisition_array, 'recall': recall_array, 'cagemate':cagemate_array, 'novel': novel_array, \n",
    "                'exposure 0': acquisition_array, 'exposure 1':exposure1_array, 'exposure 2': exposure2_array, 'exposure 3': exposure3_array}\n",
    "\n",
    "    return event_dict\n",
    "\n",
    "def p2_create_array(boris_df, times, order, min_iti, min_bout):\n",
    "    familiarization_df = boris_df[(boris_df['Start (s)'] < times[0])]\n",
    "    if order[0] == 'familiar':\n",
    "        recall_df = boris_df[(boris_df['Start (s)'] > times[3]) & (boris_df['Start (s)'] < times[4])]\n",
    "        exposure1_df = recall_df\n",
    "    if order[0] == 'cagemate':\n",
    "        cagemate_df = boris_df[(boris_df['Start (s)'] > times[3]) & (boris_df['Start (s)'] < times[4])]\n",
    "        exposure1_df = cagemate_df\n",
    "    if order[0] == 'novel':\n",
    "        novel_df = boris_df[(boris_df['Start (s)'] > times[3]) & (boris_df['Start (s)'] < times[4])]\n",
    "        exposure1_df = novel_df\n",
    "    if order[1] == 'familiar':\n",
    "        recall_df = boris_df[(boris_df['Start (s)'] > times[5]) & (boris_df['Start (s)'] < times[6])]\n",
    "        exposure2_df = recall_df \n",
    "    if order[1] == 'cagemate':\n",
    "        cagemate_df = boris_df[(boris_df['Start (s)'] > times[5]) & (boris_df['Start (s)'] < times[6])]\n",
    "        exposure2_df = cagemate_df \n",
    "    if order[1] == 'novel':\n",
    "        novel_df = boris_df[(boris_df['Start (s)'] > times[5]) & (boris_df['Start (s)'] < times[6])]\n",
    "        exposure2_df = novel_df\n",
    "    if order[2] == 'familiar':\n",
    "        recall_df = boris_df[boris_df['Start (s)'] > times[7]]\n",
    "        exposure3_df = recall_df \n",
    "    if order[2] == 'novel':\n",
    "        novel_df = boris_df[boris_df['Start (s)'] > times[7]]\n",
    "        exposure3_df = novel_df\n",
    "    if order[2] == 'cagemate':\n",
    "        cagemate_df = boris_df[boris_df['Start (s)'] > times[7]]\n",
    "        exposure3_df = cagemate_df \n",
    "    iti_array = np.array([times[1], times[2]]) #this is in seconds\n",
    "    iti_array_ms = iti_array*1000 # convert to ms\n",
    "    iti_events = random_event_generator(start =times[1], stop=times[2], len_event= 2.5, no_events=13)\n",
    "    acquisition_array = boris.get_behavior_bouts(familiarization_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    recall_array = boris.get_behavior_bouts(recall_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    cagemate_array = boris.get_behavior_bouts(cagemate_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    novel_array = boris.get_behavior_bouts(novel_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    exposure1_array = boris.get_behavior_bouts(exposure1_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    exposure2_array = boris.get_behavior_bouts(exposure2_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    exposure3_array = boris.get_behavior_bouts(exposure3_df, ['subject'], ['face sniffing', 'anogential sniffing'], min_iti, min_bout)\n",
    "    event_dict = {'acquisition': acquisition_array, 'recall': recall_array, 'cagemate':cagemate_array, 'novel': novel_array, \n",
    "                      'exposure 0': acquisition_array, 'exposure 1':exposure1_array, 'exposure 2': exposure2_array, 'exposure 3': exposure3_array,\n",
    "                      'iti': iti_array_ms.reshape(1,2), 'iti_events': iti_events}\n",
    "\n",
    "    return event_dict\n",
    "\n",
    "def p2_make_assignment(recording, subject, event_dict):\n",
    "    recording.event_dict = event_dict\n",
    "    recording.subject = subject\n",
    "\n",
    "def p2_camera_crash(boris_df1, boris_df2, times1, times2, order, media_duration, last_timestamp, min_iti, min_bout):\n",
    "    array_1 = p2_create_array(boris_df1, times1, order, min_iti, min_bout)\n",
    "    array_2 = p2_create_array(boris_df2, times2, order, min_iti, min_bout)\n",
    "    #media duration = ms: last time stamp = hz\n",
    "    # times = ms\n",
    "    diff = (last_timestamp / 20000 * 1000) - (media_duration)\n",
    "    final_dict = {}\n",
    "    for event, times in array_1.items():\n",
    "        transformed_event = array_2[event] + (diff)\n",
    "        if event != 'iti':\n",
    "            new_array = np.concatenate([array_1[event], transformed_event])\n",
    "            final_dict[event] = new_array\n",
    "        if event == 'iti':\n",
    "            transformed_iti = array_2['iti'] + diff\n",
    "            iti_array = np.concatenate([array_1['iti'], transformed_iti]).flatten()\n",
    "            iti_array = iti_array[iti_array != 0]\n",
    "            t1 = np.min(iti_array)/1000\n",
    "            t2 = np.max(iti_array)/1000\n",
    "            new_array = np.array([t1,t2])\n",
    "            #2.5 is average sniff length across acq, cage, nov, and rec\n",
    "            iti_events = random_event_generator(start= t1, stop = t2, len_event=2.5, no_events=13)\n",
    "            final_dict[event] = new_array.reshape(1,2)    \n",
    "    final_dict['iti_events'] = iti_events\n",
    "    return final_dict\n",
    "    \n",
    "def p2_camera_crash_random(times1, times2, order, media_duration2, last_timestamp, len_event, no_events):\n",
    "    diff = (last_timestamp / 20000) - (media_duration2/1000)\n",
    "    times1 = np.array(times1)\n",
    "    times2 = np.array(times2)\n",
    "    times = [0,0,0,0,0,0,0,0]\n",
    "    for i in range(len(times1)):\n",
    "        if i == len(times1) - 1:\n",
    "            times[i] = times2[i] + diff\n",
    "        else:\n",
    "            if times1[i+1] != 0:\n",
    "                if i == len(times1) - 2:\n",
    "                    times[i] = times2[i] + diff\n",
    "                else:\n",
    "                    times[i] = times1[i]\n",
    "            else:\n",
    "                times[i] = times2[i] + diff\n",
    "    event_dict = p2_create_random_array(times, order, len_event, no_events, (last_timestamp/20))\n",
    "    return event_dict  \n",
    "\n",
    "\n",
    "def assign_dicts(ephys_collection, dict_dict):\n",
    "    \"\"\"\n",
    "    Assigns behavior dictionaries to recordings in an ephys collection\n",
    "\n",
    "    Args(2):\n",
    "        ephys_collection: an ephys collection class instance\n",
    "        dict_dict: dict, a dictionary of behavior event dictionaries\n",
    "            keys: recording names \n",
    "            values: behavior event dictionaries (keys: events, values: start stop times)\n",
    "    \"\"\"\n",
    "    collection = ephys_collection.collection\n",
    "    for recording, event_dict in dict_dict.items():\n",
    "        collection[recording].event_dict = event_dict\n",
    "\n",
    "        \n",
    "def create_collection(new=False):\n",
    "    if new:\n",
    "        phase2_collection = spike.EphysRecordingCollection(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\")\n",
    "        with open('phase2collection.pkl','wb') as file:\n",
    "                pickle.dump(phase2_collection, file)\n",
    "    else:\n",
    "        try:\n",
    "            with open('phase2collection.pkl', 'rb') as file:\n",
    "                phase2_collection = pickle.load(file)\n",
    "        except FileNotFoundError:\n",
    "            phase2_collection = spike.EphysRecordingCollection(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\")\n",
    "            with open('phase2collection.pkl','wb') as file:\n",
    "                pickle.dump(phase2_collection, file)\n",
    "    return phase2_collection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#averages calculated from behavior analysis notebook for acqusition, cagemate, novel and recall\n",
    "# event length = 2.47 sec\n",
    "# event number = 12.9 ~ 13 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase2_collection = create_collection()\n",
    "FCN_1_1 = phase2_collection.get_by_name('20230803_101331_1_merged.rec')\n",
    "CNF_1_1 = phase2_collection.get_by_name('20230817_100823_1_merged.rec')\n",
    "NFC_1_1 = phase2_collection.get_by_name('20230818_115728_1_merged.rec')\n",
    "\n",
    "NFC_1_2 = phase2_collection.get_by_name('20230804_141009_1_merged.rec')\n",
    "FCN_1_2 = phase2_collection.get_by_name('20230817_113746_1_merged.rec')\n",
    "CNF_1_2 = phase2_collection.get_by_name('20230803_141047_1_merged.rec')\n",
    "                                        \n",
    "FCN_1_4 = phase2_collection.get_by_name('20230804_121600_1_merged.rec')\n",
    "NFC_1_4 = phase2_collection.get_by_name('20230803_121318_1_merged.rec')\n",
    "CNF_1_4 = phase2_collection.get_by_name('20230818_133620_1_merged.rec')\n",
    "FCN_1_1.subject = '1.1'\n",
    "CNF_1_1.subject = '1.1'\n",
    "NFC_1_1.subject = '1.1'\n",
    "NFC_1_2.subject = '1.2'\n",
    "FCN_1_2.subject = '1.2'\n",
    "CNF_1_2.subject = '1.2'\n",
    "FCN_1_4.subject = '1.4'\n",
    "NFC_1_4.subject = '1.4'\n",
    "CNF_1_4.subject = '1.4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #ONE VIDEO\n",
    "    #NEED TO DO ADD MEDIA DURAITONS FOR ALL VIDEOS REGARDLESS OF TIME CRASH \n",
    "    #20230817_100823_1.1_CNF\n",
    "    CNF_1_1_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230817_100823_1_merged.rec\\\\20230817_100823_1.1_CNF.xlsx\")\n",
    "    times_CNF_1_1 = [630, 633, 1226, 1228, 1526, 1531, 1828, 1832]\n",
    "    order_CNF_1_1 = ['cagemate', 'novel', 'familiar']\n",
    "    CNF_1_1_media_duration = 2131.233 * 1000\n",
    "    CNF_1_1_last_timestamp = CNF_1_1.timestamps_var[-1]\n",
    "\n",
    "\n",
    "    #TWO VIDEOS\n",
    "    #20230803_101331_1_FCN_1\n",
    "    FCN_1_1_1_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230803_101331_1_merged.rec\\\\20230803 101331 1.1.xlsx\")\n",
    "    FCN_1_1_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\Phase 2\\\\20230803_101331_1_merged.rec\\\\20230803 101331 1.2.xlsx\")\n",
    "    times_FCN_1_1_1 = [599, 603, 821, 0, 0, 0, 0, 99999999999]\n",
    "    times_FCN_1_1_2 = [0, 0, 374, 377, 671, 677, 970, 976]\n",
    "    order_FCN_1_1 = ['familiar', 'cagemate', 'novel']\n",
    "    FCN_1_1_media_duration = 1290.567 * 1000\n",
    "    FCN_1_1_last_timestamp = FCN_1_1.timestamps_var[-1]\n",
    "    FCN_1_1_media_duration1 = 821.200 * 1000\n",
    "\n",
    "\n",
    "    #20230818_115728_1.1_NFC\n",
    "    NFC_1_1_1_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230818_115728_1_merged.rec\\\\20230818 115728 1.1.xlsx\")\n",
    "    NFC_1_1_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230818_115728_1_merged.rec\\\\20230818 115728 1.2.xlsx\")\n",
    "    times_NFC_1_1_1 = [111, 0, 0, 0, 0, 0, 0, 99999999999]\n",
    "    times_NFC_1_1_2 = [527, 533, 1147, 1151, 1446, 1454, 1754, 1762]\n",
    "    order_NFC_1_1 = ['novel', 'familiar', 'cagemate']\n",
    "    NFC_1_1_media_duration = 2059.6 * 1000\n",
    "    NFC_1_1_last_timestamp = NFC_1_1.timestamps_var[-1]\n",
    "    NFC_1_1_media_duration1 = 115.667 * 1000\n",
    "\n",
    "    #20230704_141009_1.2_NFC8\n",
    "    NFC_1_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230804_141009_1_merged.rec\\\\20230804_141009_1.2_2t2bL_NFC.xlsx\")\n",
    "    times_NFC_1_2 = [600, 606, 1199, 1202, 1500, 1508, 1800, 1806]\n",
    "    order_NFC_1_2 = ['novel', 'familiar', 'cagemate']\n",
    "    NFC_1_2_media_duration  = 2106.100 * 1000\n",
    "    NFC_1_2_last_timestamp = NFC_1_2.timestamps_var[-1]\n",
    "\n",
    "\n",
    "\n",
    "    #20230817_113746_1.2_FCN\n",
    "    FCN_1_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230817_113746_1_merged.rec\\\\20230817_113746_1.2_2t2bL_FCN.xlsx\")\n",
    "    times_FCN_1_2 = [599, 604, 1200, 1203, 1499, 1506, 1811, 1817]\n",
    "    order_FCN_1_2 = ['familiar', 'cagemate', 'novel']\n",
    "    FCN_1_2_media_duration = 2110.67 * 1000\n",
    "    FCN_1_2_last_timestamp = FCN_1_2.timestamps_var[-1]\n",
    "\n",
    "\n",
    "\n",
    "    #20230803_141047_1.2_CNF\n",
    "    CNF_1_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230803_141047_1_merged.rec\\\\20230803 _ 141047 _1.2 2t2bL CNF .xlsx\")\n",
    "    times_CNF_1_2 = [600, 605, 1200, 1203, 1500, 1506, 1800, 1806]\n",
    "    order_CNF_1_2 = ['cagemate', 'novel', 'familiar']\n",
    "    CNF_1_2_media_duration = 2101.170 * 1000\n",
    "    CNF_1_2_last_timestamp = CNF_1_2.timestamps_var[-1]\n",
    "\n",
    "\n",
    "    #20230804_121600_1.4_FCN\n",
    "    FCN_1_4_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230804_121600_1_merged.rec\\\\20230804_121600_1.4_3t3bL_FCN.xlsx\")\n",
    "    times_FCN_1_4 = [600, 607, 1203, 1206, 1499, 1505, 1799, 1807]\n",
    "    order_FCN_1_4 = ['familiar', 'cagemate', 'novel']\n",
    "    FCN_1_4_media_duration = 2101.00 * 1000\n",
    "    FCN_1_4_last_timestamp = FCN_1_4.timestamps_var[-1]\n",
    "\n",
    "\n",
    "    #20230818_133620_1.4_CNF\n",
    "    CNF_1_4_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230818_133620_1_merged.rec\\\\20230818_133620_1.4_3t3bL_CNF.xlsx\")\n",
    "    times_CNF_1_4 = [599, 605, 1158, 1202, 1497, 1503, 1800, 1806]\n",
    "    order_CNF_1_4 = ['cagemate', 'novel', 'familiar']\n",
    "    CNF_1_4_media_duration = 2100.333 * 1000\n",
    "    CNF_1_4_last_timestamp = CNF_1_4.timestamps_var[-1]\n",
    "\n",
    "    #20230803_121318_1.4_NFC\n",
    "    NFC_1_4_1_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230803_121318_1_merged.rec\\\\20230803 121318 1.1.xlsx\")\n",
    "    NFC_1_4_2_boris_df = pd.read_excel(\"D:\\\\social_ephys_pilot2_cum\\\\proc\\\\Phase 2\\\\20230803_121318_1_merged.rec\\\\20230803 121318 1.2.xlsx\")\n",
    "    times_NFC_1_4_1 = [599, 604, 1021, 0, 0, 0, 0, 99999999]\n",
    "    times_NFC_1_4_2 =  [0, 0, 50, 53, 347, 354, 650, 656]\n",
    "    order_NFC_1_4 = ['novel', 'familiar', 'cagemate']\n",
    "    NFC_1_4_media_duration = 951.633 * 1000\n",
    "    NFC_1_4_last_timestamp = NFC_1_4.timestamps_var[-1]\n",
    "    NFC_1_4_media_duration1 = 1021.333 * 1000\n",
    "except FileNotFoundError:\n",
    "    print('File not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera crash\n"
     ]
    }
   ],
   "source": [
    "min_iti = 1\n",
    "try:     \n",
    "    sniff_dict = {}\n",
    "    CNF_1_1_arrays = p2_create_array(CNF_1_1_boris_df, times_CNF_1_1, order_CNF_1_1, min_iti, 0.5)\n",
    "    sniff_dict['20230817_100823_1_merged.rec'] = CNF_1_1_arrays\n",
    "\n",
    "    FCN_1_1_arrays = p2_camera_crash(FCN_1_1_1_boris_df, FCN_1_1_2_boris_df, times_FCN_1_1_1, times_FCN_1_1_2, order_FCN_1_1, FCN_1_1_media_duration, FCN_1_1_last_timestamp, min_iti, 0.5)\n",
    "    sniff_dict['20230803_101331_1_merged.rec'] = FCN_1_1_arrays\n",
    "\n",
    "    NFC_1_1_arrays = p2_camera_crash(NFC_1_1_1_boris_df, NFC_1_1_2_boris_df, times_NFC_1_1_1, times_NFC_1_1_2, order_NFC_1_1, NFC_1_1_media_duration, NFC_1_1_last_timestamp, min_iti, 0.5)\n",
    "    sniff_dict['20230818_115728_1_merged.rec'] = NFC_1_1_arrays\n",
    "\n",
    "    NFC_1_2_arrays = p2_create_array(NFC_1_2_boris_df, times_NFC_1_2, order_NFC_1_2, min_iti, 0.5)\n",
    "    sniff_dict['20230804_141009_1_merged.rec'] = NFC_1_2_arrays \n",
    "\n",
    "    FCN_1_2_arrays = p2_create_array(FCN_1_2_boris_df, times_FCN_1_2, order_FCN_1_2, min_iti, 0.5)\n",
    "    sniff_dict['20230817_113746_1_merged.rec'] = FCN_1_2_arrays\n",
    "\n",
    "    CNF_1_2_arrays = p2_create_array(CNF_1_2_boris_df, times_CNF_1_2, order_CNF_1_2, min_iti, 0.5)\n",
    "    sniff_dict['20230803_141047_1_merged.rec'] = CNF_1_2_arrays\n",
    "\n",
    "    FCN_1_4_arrays = p2_create_array(FCN_1_4_boris_df, times_FCN_1_4, order_FCN_1_4, min_iti, 0.5)\n",
    "    sniff_dict['20230804_121600_1_merged.rec'] = FCN_1_4_arrays\n",
    "\n",
    "    CNF_1_4_arrays = p2_create_array(CNF_1_4_boris_df, times_CNF_1_4, order_CNF_1_4, min_iti, 0.5)\n",
    "    sniff_dict['20230818_133620_1_merged.rec'] = CNF_1_4_arrays\n",
    "\n",
    "    NFC_1_4_arrays = p2_camera_crash(NFC_1_4_1_boris_df, NFC_1_4_2_boris_df, times_NFC_1_4_1, times_NFC_1_4_2, order_NFC_1_4, NFC_1_4_media_duration, NFC_1_4_last_timestamp, min_iti, 0.5)\n",
    "    sniff_dict['20230803_121318_1_merged.rec'] = NFC_1_4_arrays\n",
    "except FileNotFoundError:\n",
    "    print('File not found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_this(sniff_dict, 'event_dict_phase2.pkl')\n",
    "assign_dicts(phase2_collection, sniff_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 2)\n",
      "[638000. 640500.]\n",
      "[1220500. 1223000.]\n",
      "(13, 2)\n",
      "[633000. 635500.]\n",
      "[1180500. 1183000.]\n",
      "(13, 2)\n",
      "[683430.4 685930.4]\n",
      "[1253430.4 1255930.4]\n",
      "(13, 2)\n",
      "[706000. 708500.]\n",
      "[1158500. 1161000.]\n",
      "(13, 2)\n",
      "[669000. 671500.]\n",
      "[1144000. 1146500.]\n",
      "(13, 2)\n",
      "[622500. 625000.]\n",
      "[1197500. 1200000.]\n",
      "(13, 2)\n",
      "[644500. 647000.]\n",
      "[1174500. 1177000.]\n",
      "(13, 2)\n",
      "[627500. 630000.]\n",
      "[1145000. 1147500.]\n",
      "(13, 2)\n",
      "[631500. 634000.]\n",
      "[1149000. 1151500.]\n"
     ]
    }
   ],
   "source": [
    "for recording, dict in sniff_dict.items():\n",
    "    print(dict['iti_events'].shape)\n",
    "    print(dict['iti_events'][0])\n",
    "    print(dict['iti_events'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCN_1_1.order = ['recall', 'cagemate', 'novel']\n",
    "CNF_1_1.order = ['cagemate', 'novel', 'recall']\n",
    "NFC_1_1.order = ['novel', 'recall', 'cagemate']\n",
    "NFC_1_2.order = ['novel', 'recall', 'cagemate']\n",
    "FCN_1_2.order = ['recall', 'cagemate', 'novel']\n",
    "CNF_1_2.order = ['cagemate', 'novel', 'recall']\n",
    "FCN_1_4.order = ['recall', 'cagemate', 'novel']\n",
    "NFC_1_4.order = ['novel', 'recall', 'cagemate']\n",
    "CNF_1_4.order = ['cagemate', 'novel', 'recall']\n",
    "\n",
    " \n",
    "for name, recording in phase2_collection.collection.items():\n",
    "        order_dict = {}\n",
    "        exposure = [\"exposure 1\", \"exposure 2\", 'exposure 3']\n",
    "        for i in range(len(recording.order)):\n",
    "                order_dict[recording.order[i]] = exposure[i]\n",
    "        recording.order_dict = order_dict\n",
    " \n",
    "for name, recording in phase2_collection.collection.items():\n",
    "        reverse_order_dict = {}\n",
    "        exposure = [\"exposure 1\", \"exposure 2\", 'exposure 3']\n",
    "        for i in range(len(recording.order)):\n",
    "                reverse_order_dict[exposure[i]] = recording.order[i]\n",
    "        recording.reverse_order_dict = reverse_order_dict\n",
    "\n",
    "pickle_this(phase2_collection, 'phase2collection.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
