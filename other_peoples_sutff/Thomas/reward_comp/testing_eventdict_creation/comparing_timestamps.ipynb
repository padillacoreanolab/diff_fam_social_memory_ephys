{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32607b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spike.spike_analysis.spike_collection as sc\n",
    "import spike.spike_analysis.spike_recording as sr\n",
    "import spike.spike_analysis.firing_rate_calculations as fr\n",
    "import spike.spike_analysis.normalization as norm\n",
    "import spike.spike_analysis.single_cell as single_cell\n",
    "import spike.spike_analysis.spike_collection as collection\n",
    "import spike.spike_analysis.zscoring as zscoring\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import behavior.boris_extraction as boris\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3580e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)  # 0 means unlimited in newer pandas versions\n",
    "\n",
    "# Show all rows\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Donâ€™t truncate column contents\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Expand the display to the full width of the screen\n",
    "pd.set_option(\"display.width\", 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c20f645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_collection_json_path = r'C:\\Users\\thoma\\Code\\ResearchCode\\diff_fam_social_memory_ephys\\spike_collection.json\\spike_collection.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa319d3",
   "metadata": {},
   "source": [
    "### Loading spike Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44ff3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sc.SpikeCollection.load_collection(spike_collection_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0137222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique event names: ['alone_rewarded', 'alone_rewarded_baseline', 'high_comp', 'high_comp_lose', 'high_comp_lose_baseline', 'high_comp_win', 'high_comp_win_baseline', 'lose', 'low_comp', 'low_comp_lose', 'low_comp_lose_baseline', 'low_comp_win', 'low_comp_win_baseline', 'overall_pretone', 'win']\n"
     ]
    }
   ],
   "source": [
    "rec_events = sp.recordings[0].event_dict\n",
    "\n",
    "# get unique event names from rec_events dictionary\n",
    "event_names = list(rec_events.keys())\n",
    "print(\"Unique event names:\", event_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2e9b0",
   "metadata": {},
   "source": [
    "### Loading Alternative Event Dict and Original (with frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418d45b",
   "metadata": {},
   "source": [
    "### Alternative (Taylors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53a63671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['20230612_101430_standard_comp_to_training_D1_subj_1-3', '20230612_101430_standard_comp_to_training_D1_subj_1-4', '20230612_112630_standard_comp_to_training_D1_subj_1-1', '20230612_112630_standard_comp_to_training_D1_subj_1-2', '20230613_105657_standard_comp_to_training_D2_subj_1-1', '20230613_105657_standard_comp_to_training_D2_subj_1-4', '20230614_114041_standard_comp_to_training_D3_subj_1-1', '20230614_114041_standard_comp_to_training_D3_subj_1-2', '20230616_111904_standard_comp_to_training_D4_subj_1-2', '20230616_111904_standard_comp_to_training_D4_subj_1-4', '20230617_115521_standard_comp_to_omission_D1_subj_1-1', '20230617_115521_standard_comp_to_omission_D1_subj_1-2', '20230618_100636_standard_comp_to_omission_D2_subj_1-1', '20230618_100636_standard_comp_to_omission_D2_subj_1-4', '20230619_115321_standard_comp_to_omission_D3_subj_1-2', '20230619_115321_standard_comp_to_omission_D3_subj_1-4', '20230620_114347_standard_comp_to_omission_D4_subj_1-1', '20230620_114347_standard_comp_to_omission_D4_subj_1-2', '20230621_111240_standard_comp_to_omission_D5_subj_1-2', '20230621_111240_standard_comp_to_omission_D5_subj_1-4', '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-1', '20230622_110832_standard_comp_to_both_rewarded_D1_subj_1-2', '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-2', '20230624_105855_standard_comp_to_both_rewarded_D3_subj_1-4', '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-1', '20230625_112913_standard_comp_to_both_rewarded_D4_subj_1-4', '20240320_142408_alone_comp_subj_3-1', '20240320_142408_alone_comp_subj_3-3', '20240320_171038_alone_comp_subj_4-2', '20240320_171038_alone_comp_subj_4-3', '20240322_120625_alone_comp_subj_3-3', '20240322_120625_alone_comp_subj_3-4', '20240322_160946_alone_comp_subj_4-3', '20240322_160946_alone_comp_subj_4-4', '20240323_122227_alone_comp_subj_5-2', '20240323_122227_alone_comp_subj_5-3', '20240323_144517_alone_comp_subj_3-1', '20240323_144517_alone_comp_subj_3-4', '20240323_165815_alone_comp_subj_4-2', '20240323_165815_alone_comp_subj_4-4'])\n"
     ]
    }
   ],
   "source": [
    "# Load the alternative event dict\n",
    "alternative_event_dict_path = r'C:\\Users\\thoma\\Code\\ResearchCode\\diff_fam_social_memory_ephys\\other_peoples_sutff\\Thomas\\reward_comp\\testing_eventdict_creation\\alternative_event_dict_20251003_082057.pkl'\n",
    "\n",
    "with open(alternative_event_dict_path, 'rb') as f:\n",
    "    alternative_event_dict = pickle.load(f)\n",
    "\n",
    "print(alternative_event_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e6720",
   "metadata": {},
   "source": [
    "#### Originalish (Frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a843c1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec', '20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec', '20230612_112630_standard_comp_to_training_D1_subj_1-2_t2b2L_box1_merged.rec', '20230612_112630_standard_comp_to_training_D1_subj_1-1_t1b3L_box2_merged.rec', '20230613_105657_standard_comp_to_training_D2_subj_1-1_t1b2L_box1_merged.rec', '20230613_105657_standard_comp_to_training_D2_subj_1-4_t4b3L_box2_merged.rec', '20230614_114041_standard_comp_to_training_D3_subj_1-1_t1b3L_box1_merged.rec', '20230614_114041_standard_comp_to_training_D3_subj_1-2_t2b2L_box2_merged.rec', '20230616_111904_standard_comp_to_training_D4_subj_1-4_t4b3L_box1_merged.rec', '20230616_111904_standard_comp_to_training_D4_subj_1-2_t2b2L_box2_merged.rec', '20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec', '20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1-4_t4b3L_box1_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1-1_t1b2L_box2_merged.rec', '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec', '20240320_142408_alone_comp_subj_3-1_t6b6_merged.rec', '20240320_142408_alone_comp_subj_3-3_t5b5_merged.rec', '20240320_171038_alone_comp_subj_4-2_t6b6_merged.rec', '20240320_171038_alone_comp_subj_4-3_t5b5_merged.rec', '20240322_120625_alone_comp_subj_3-3_t6b6_merged.rec', '20240322_120625_alone_comp_subj_3-4_t5b5_merged.rec', '20240322_160946_alone_comp_subj_4-3_t6b6_merged.rec', '20240322_160946_alone_comp_subj_4-4_t5b5_merged.rec', '20240323_122227_alone_comp_subj_5-2_t6b6_merged.rec', '20240323_122227_alone_comp_subj_5-3_t5b5_merged.rec', '20240323_144517_alone_comp_subj_3-1_t5b5_merged.rec', '20240323_144517_alone_comp_subj_3-4_t6b6_merged.rec', '20240323_165815_alone_comp_subj_4-2_t5b5_merged.rec', '20240323_165815_alone_comp_subj_4-4_t6b6_merged.rec'])\n"
     ]
    }
   ],
   "source": [
    "# Load the original event dict\n",
    "original_event_dict_path = r'C:\\Users\\thoma\\Code\\ResearchCode\\diff_fam_social_memory_ephys\\other_peoples_sutff\\Thomas\\reward_comp\\testing_eventdict_creation\\original_event_dict_20251003_082057.pkl'\n",
    "\n",
    "with open(original_event_dict_path, 'rb') as f:\n",
    "    original_event_dict = pickle.load(f)\n",
    "\n",
    "print(original_event_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52c2e2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in test_rec: ['rewarded', 'omission', 'both_rewarded', 'tie', 'no_comp_win', 'no_comp_lose', 'competitive_win', 'competitive_lose']\n",
      "\n",
      "Sample event structure:\n",
      "Event 'rewarded':\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Sample values: []\n"
     ]
    }
   ],
   "source": [
    "test_rec = alternative_event_dict[\"20230612_101430_standard_comp_to_training_D1_subj_1-3\"]\n",
    "\n",
    "# Let's examine the structure of an event dictionary entry\n",
    "print(\"Keys in test_rec:\", list(test_rec.keys()))\n",
    "print(\"\\nSample event structure:\")\n",
    "sample_event_key = list(test_rec.keys())[0]\n",
    "print(f\"Event '{sample_event_key}':\")\n",
    "print(f\"Type: {type(test_rec[sample_event_key])}\")\n",
    "print(f\"Sample values: {test_rec[sample_event_key][:10] if hasattr(test_rec[sample_event_key], '__len__') else test_rec[sample_event_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11435b07",
   "metadata": {},
   "source": [
    "#### Looking at Timestamps max time value to compare to spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b823eb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Find maximum values across all events in both dictionaries\\n\\ndef find_max_values_in_event_dict(event_dict, dict_name):\\n    \"\"\"Find the maximum value across all events in all recordings in an event dictionary\"\"\"\\n    all_values = []\\n    \\n    \\n    for recording_name, recording_events in event_dict.items():        \\n        for event_name, event_values in recording_events.items():\\n            if len(event_values) > 0:  # Only process non-empty events\\n                max_val = np.max(event_values)\\n                all_values.extend(event_values)\\n            else:\\n                print(f\"  {event_name}: empty\")\\n    \\n    if all_values:\\n        overall_max = np.max(all_values)\\n        return overall_max, all_values\\n    else:\\n        print(f\"\\nNo values found in {dict_name}\")\\n        return None, []\\n\\n# Find max values in alternative event dict\\nalt_max, alt_values = find_max_values_in_event_dict(alternative_event_dict, \"Alternative Event Dict\")\\n\\n# Find max values in original event dict  \\norig_max, orig_values = find_max_values_in_event_dict(original_event_dict, \"Original Event Dict\")\\n\\n# Compare the overall maximums\\nprint(f\"\\n=== SUMMARY ===\")\\nif alt_max is not None:\\n    print(f\"Alternative Event Dict - Overall Max: {alt_max}\")\\nif orig_max is not None:\\n    print(f\"Original Event Dict - Overall Max: {orig_max}\")\\n\\nif alt_max is not None and orig_max is not None:\\n    print(f\"Difference: {abs(alt_max - orig_max)}\")\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Find maximum values across all events in both dictionaries\n",
    "\n",
    "def find_max_values_in_event_dict(event_dict, dict_name):\n",
    "    \"\"\"Find the maximum value across all events in all recordings in an event dictionary\"\"\"\n",
    "    all_values = []\n",
    "    \n",
    "    \n",
    "    for recording_name, recording_events in event_dict.items():        \n",
    "        for event_name, event_values in recording_events.items():\n",
    "            if len(event_values) > 0:  # Only process non-empty events\n",
    "                max_val = np.max(event_values)\n",
    "                all_values.extend(event_values)\n",
    "            else:\n",
    "                print(f\"  {event_name}: empty\")\n",
    "    \n",
    "    if all_values:\n",
    "        overall_max = np.max(all_values)\n",
    "        return overall_max, all_values\n",
    "    else:\n",
    "        print(f\"\\nNo values found in {dict_name}\")\n",
    "        return None, []\n",
    "\n",
    "# Find max values in alternative event dict\n",
    "alt_max, alt_values = find_max_values_in_event_dict(alternative_event_dict, \"Alternative Event Dict\")\n",
    "\n",
    "# Find max values in original event dict  \n",
    "orig_max, orig_values = find_max_values_in_event_dict(original_event_dict, \"Original Event Dict\")\n",
    "\n",
    "# Compare the overall maximums\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "if alt_max is not None:\n",
    "    print(f\"Alternative Event Dict - Overall Max: {alt_max}\")\n",
    "if orig_max is not None:\n",
    "    print(f\"Original Event Dict - Overall Max: {orig_max}\")\n",
    "\n",
    "if alt_max is not None and orig_max is not None:\n",
    "    print(f\"Difference: {abs(alt_max - orig_max)}\")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4cd173",
   "metadata": {},
   "source": [
    "last event happens at 2.5 min? can't be right. data later on shows this probably isn't the case but check back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5119a8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155893.05000000002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3117861 * (1000 / 20000) # converting largest timestamp to ms to see where the last event happened in ms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc547584",
   "metadata": {},
   "source": [
    "### Dropping recs in sp.event_dict that aren't in alt event dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fb5ff8",
   "metadata": {},
   "source": [
    "### new mapped code | Dropping recs in sp.event_dict that aren't in alt event dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75615151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zscore_global_baseline_mapped(recording, event_name, alt_event_dict, recording_mapping, pre_window=10, SD=1.65, verbose=False):\n",
    "    \"\"\"\n",
    "    Z-score event firing rates using a *pooled* baseline (all event types) per unit.\n",
    "    This function calculates the z-score of firing rates for a specific event type\n",
    "    based on a global baseline computed from all event types in the recording.\n",
    "    \n",
    "    MODIFIED: Uses recording mapping to match spike collection names to alt event dict keys\n",
    "    \n",
    "    Parameters:\n",
    "    - recording: SpikeRecording object containing spike data and events.\n",
    "    - event_name: Name of the event type to analyze.\n",
    "    - alt_event_dict: Alternative event dictionary to use instead of recording.event_dict\n",
    "    - recording_mapping: Dictionary mapping spike collection names to alt event dict keys\n",
    "    - pre_window: Duration in seconds before the event to use for baseline calculation.\n",
    "    - SD: Number of standard deviations to use for significance thresholding.\n",
    "    - verbose: If True, prints additional information during processing.\n",
    "    Returns:\n",
    "    - A pandas DataFrame containing the z-scores and significance of firing rates for each unit\n",
    "    for the specified event type.\n",
    "    \"\"\"\n",
    "    # Check if recording has a mapping to alt_event_dict\n",
    "    alt_recording_name = recording_mapping.get(recording.name)\n",
    "    if alt_recording_name is None or alt_recording_name not in alt_event_dict:\n",
    "        if verbose:\n",
    "            print(f\"Recording {recording.name} -> {alt_recording_name} not found in alternative event dict. Skipping.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Use the alternative event dict for this recording\n",
    "    recording_events = alt_event_dict[alt_recording_name]\n",
    "    \n",
    "    # Step 1: Pool all baseline windows across all events for each unit\n",
    "    global_baseline_counts = {}\n",
    "    units = getattr(recording, \"good_units\", None) # get good units if available\n",
    "    if units is None: # if not, use labels_dict\n",
    "        units = [unit_id for unit_id, label in recording.labels_dict.items() if label == \"good\"]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Using labels_dict to determine good units.\")\n",
    "            print(f\"Good units found: {units}\\n\\nFrom labels_dict: {recording.labels_dict}\\n\\n\")\n",
    "\n",
    "    # Initialize global baseline list per unit\n",
    "    for unit_id in units:\n",
    "        global_baseline_counts[unit_id] = []\n",
    "    if verbose:\n",
    "        print(f\"Global baseline counts initialized for units: {global_baseline_counts}\\n\")    \n",
    "\n",
    "    # Loop through all event types and pool all baselines\n",
    "    # creates a list of baseline counts for each unit\n",
    "    for ev_type, event_timestamps in recording_events.items():\n",
    "        # Convert single timestamps to windows if needed\n",
    "        if len(event_timestamps) > 0 and not isinstance(event_timestamps[0], (list, tuple)):\n",
    "            # Assume these are single timestamps, create 1-second windows\n",
    "            event_windows = [[ts, ts + 1000] for ts in event_timestamps]\n",
    "        else:\n",
    "            event_windows = event_timestamps\n",
    "            \n",
    "        for unit_id in units:\n",
    "            spikes = recording.unit_timestamps[unit_id] # array of spike timestamps\n",
    "            spikes_ms = spikes * (1000 / recording.sampling_rate) # convert to milliseconds since event_windows are in ms\n",
    "\n",
    "            for window in event_windows:\n",
    "                if len(window) >= 2:\n",
    "                    start_event = window[0]\n",
    "                    start_baseline = start_event - int(pre_window * 1000)\n",
    "                    end_baseline = start_event\n",
    "                    baseline_count = np.sum((spikes_ms >= start_baseline) & (spikes_ms < end_baseline))\n",
    "                    global_baseline_counts[unit_id].append(baseline_count) # list of counts for this unit appended to global_baseline_counts\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Unit {unit_id}, Event {ev_type}, Baseline count: {baseline_count}\")\n",
    "\n",
    "    # Step 2: Compute global baseline mean and SD per unit using numpy\n",
    "    baseline_mean = {u: np.mean(c) for u, c in global_baseline_counts.items()}\n",
    "    baseline_sd = {u: np.std(c) for u, c in global_baseline_counts.items()}\n",
    "\n",
    "    # Step 3: For the target event, calculate z-scores\n",
    "    if event_name not in recording_events:\n",
    "        if verbose:\n",
    "            print(f\"Event {event_name} not found in recording {alt_recording_name}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    event_timestamps = recording_events[event_name]\n",
    "    \n",
    "    # Convert single timestamps to windows if needed\n",
    "    if len(event_timestamps) > 0 and not isinstance(event_timestamps[0], (list, tuple)):\n",
    "        # Assume these are single timestamps, create 1-second windows\n",
    "        event_windows = [[ts, ts + 1000] for ts in event_timestamps]\n",
    "    else:\n",
    "        event_windows = event_timestamps\n",
    "    \n",
    "    event_firing = {}\n",
    "    rows = []\n",
    "    for unit_id in units:\n",
    "        spikes = recording.unit_timestamps[unit_id]\n",
    "        spikes_ms = spikes * (1000 / recording.sampling_rate)\n",
    "        event_counts = []\n",
    "        for window in event_windows:\n",
    "            if len(window) >= 2:\n",
    "                start_event = window[0]\n",
    "                end_event = window[1]\n",
    "                event_count = np.sum((spikes_ms >= start_event) & (spikes_ms < end_event)) # count spikes in the event window using masking\n",
    "                event_counts.append(event_count)\n",
    "\n",
    "        # getting all the important values for z-score calculation per unit\n",
    "        ev_mean = np.mean(event_counts) if event_counts else 0\n",
    "        b_mean = baseline_mean[unit_id]\n",
    "        b_sd = baseline_sd[unit_id]\n",
    "\n",
    "        # Calculate z-score\n",
    "        zscore = np.nan if b_sd == 0 else (ev_mean - b_mean) / b_sd \n",
    "\n",
    "        rows.append({\n",
    "            \"Recording\": recording.name,  # Keep original spike collection name\n",
    "            \"Alt_Recording\": alt_recording_name,  # Add mapped name for reference\n",
    "            \"Event name\": event_name,\n",
    "            \"Unit number\": unit_id,\n",
    "            \"Global Pre-event M\": b_mean,\n",
    "            \"Global Pre-event SD\": b_sd,\n",
    "            \"Event M\": ev_mean,\n",
    "            \"Event Z-Score\": zscore,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Step 4: Apply significance threshold\n",
    "    conditions = [\n",
    "        (df['Event Z-Score'] > SD),\n",
    "        (df['Event Z-Score'] < -SD)]\n",
    "\n",
    "    values = ['increase', 'decrease']\n",
    "\n",
    "    # Apply the conditions\n",
    "    df['sig'] = np.select(conditions, values, default='not sig')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def run_all_recordings_all_events_mapped(sp, alt_event_dict, recording_mapping, pre_window=10, SD=1.65, verbose=False):\n",
    "    \"\"\"\n",
    "    Modified version that uses recording mapping to match spike collection names to alt event dict keys\n",
    "    \"\"\"\n",
    "    # Get event names from alt_event_dict instead of sp.recordings\n",
    "    all_event_names = set()\n",
    "    for recording_name, recording_events in alt_event_dict.items():\n",
    "        all_event_names.update(recording_events.keys())\n",
    "    all_event_names = sorted(all_event_names)\n",
    "    \n",
    "    dfs = []\n",
    "    skipped = []\n",
    "\n",
    "    for rec in sp.recordings:\n",
    "        # Check if recording has a valid mapping\n",
    "        alt_recording_name = recording_mapping.get(rec.name)\n",
    "        if alt_recording_name is None or alt_recording_name not in alt_event_dict:\n",
    "            skipped.append((rec.name, \"ALL_EVENTS\", \"no mapping to alt_event_dict\"))\n",
    "            if verbose:\n",
    "                print(f\"Skipping recording {rec.name} - no valid mapping to alternative event dict\")\n",
    "            continue\n",
    "            \n",
    "        recording_events = alt_event_dict[alt_recording_name]\n",
    "        \n",
    "        for ev in all_event_names:\n",
    "            if ev not in recording_events or len(recording_events[ev]) == 0:\n",
    "                skipped.append((rec.name, ev, \"no events in alt_event_dict\"))\n",
    "                continue\n",
    "            try:\n",
    "                df_ev = run_zscore_global_baseline_mapped(rec, ev, alt_event_dict, recording_mapping, pre_window=pre_window, SD=SD, verbose=verbose)\n",
    "                if not df_ev.empty:\n",
    "                    # Extra metadata, if you want it\n",
    "                    df_ev[\"Event windows\"] = len(recording_events[ev])\n",
    "                    dfs.append(df_ev)\n",
    "                else:\n",
    "                    skipped.append((rec.name, ev, \"empty result\"))\n",
    "            except Exception as e:\n",
    "                skipped.append((rec.name, ev, f\"error: {e}\"))\n",
    "                if verbose:\n",
    "                    print(f\"[WARN] {rec.name} / {ev} -> {e}\")\n",
    "\n",
    "    big_df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "    return big_df, pd.DataFrame(skipped, columns=[\"Recording\", \"Event name\", \"reason\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadcce1b",
   "metadata": {},
   "source": [
    "### new testing of mapped funciton |  mapping merged.recs in sp to recs in alt event dict that doesn't include the extra info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c029628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording name mapping created successfully!\n",
      "Total mappings: 39\n",
      "Successfully matched: 29\n",
      "No matches found: 10\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping between spike collection recording names and alt event dict keys\n",
    "def create_recording_name_mapping(sp_recordings, alt_event_dict_keys):\n",
    "    \"\"\"\n",
    "    Create a mapping between spike collection recording names and alternative event dict keys\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    \n",
    "    for sp_rec in sp_recordings:\n",
    "        sp_name = sp_rec.name\n",
    "        \n",
    "        # Try to find a match in alt_event_dict_keys\n",
    "        best_match = None\n",
    "        for alt_key in alt_event_dict_keys:\n",
    "            # Check if the alt_key is a substring of the sp_name\n",
    "            if alt_key in sp_name:\n",
    "                best_match = alt_key\n",
    "                break\n",
    "        \n",
    "        if best_match:\n",
    "            mapping[sp_name] = best_match\n",
    "        else:\n",
    "            mapping[sp_name] = None\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "# Create the mapping\n",
    "recording_mapping = create_recording_name_mapping(sp.recordings, alternative_event_dict.keys())\n",
    "\n",
    "print(\"Recording name mapping created successfully!\")\n",
    "print(f\"Total mappings: {len(recording_mapping)}\")\n",
    "matched_count = sum(1 for v in recording_mapping.values() if v is not None)\n",
    "print(f\"Successfully matched: {matched_count}\")\n",
    "print(f\"No matches found: {len(recording_mapping) - matched_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e4803e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the new mapping-based functions:\n",
      "\n",
      "Recordings that will be processed (with mapping):\n",
      "  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec -> 20230612_101430_standard_comp_to_training_D1_subj_1-3\n",
      "  20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec -> 20230612_101430_standard_comp_to_training_D1_subj_1-4\n",
      "  20230612_112630_standard_comp_to_training_D1_subj_1-1_t1b3L_box2_merged.rec -> 20230612_112630_standard_comp_to_training_D1_subj_1-1\n",
      "  20230612_112630_standard_comp_to_training_D1_subj_1-2_t2b2L_box1_merged.rec -> 20230612_112630_standard_comp_to_training_D1_subj_1-2\n",
      "  20230613_105657_standard_comp_to_training_D2_subj_1-1_t1b2L_box1_merged.rec -> 20230613_105657_standard_comp_to_training_D2_subj_1-1\n",
      "  20230613_105657_standard_comp_to_training_D2_subj_1-4_t4b3L_box2_merged.rec -> 20230613_105657_standard_comp_to_training_D2_subj_1-4\n",
      "  20230614_114041_standard_comp_to_training_D3_subj_1-1_t1b3L_box1_merged.rec -> 20230614_114041_standard_comp_to_training_D3_subj_1-1\n",
      "  20230614_114041_standard_comp_to_training_D3_subj_1-2_t2b2L_box2_merged.rec -> 20230614_114041_standard_comp_to_training_D3_subj_1-2\n",
      "  20230616_111904_standard_comp_to_training_D4_subj_1-2_t2b2L_box2_merged.rec -> 20230616_111904_standard_comp_to_training_D4_subj_1-2\n",
      "  20230616_111904_standard_comp_to_training_D4_subj_1-4_t4b3L_box1_merged.rec -> 20230616_111904_standard_comp_to_training_D4_subj_1-4\n",
      "  20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec -> 20230617_115521_standard_comp_to_omission_D1_subj_1-1\n",
      "  20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec -> 20230617_115521_standard_comp_to_omission_D1_subj_1-2\n",
      "  20230618_100636_standard_comp_to_omission_D2_subj_1-1_t1b2L_box2_merged.rec -> 20230618_100636_standard_comp_to_omission_D2_subj_1-1\n",
      "  20230618_100636_standard_comp_to_omission_D2_subj_1-4_t4b3L_box1_merged.rec -> 20230618_100636_standard_comp_to_omission_D2_subj_1-4\n",
      "  20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec -> 20230619_115321_standard_comp_to_omission_D3_subj_1-4\n",
      "  20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec -> 20230620_114347_standard_comp_to_omission_D4_subj_1-1\n",
      "  20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec -> 20230620_114347_standard_comp_to_omission_D4_subj_1-2\n",
      "  20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec -> 20230621_111240_standard_comp_to_omission_D5_subj_1-4\n",
      "  20240320_142408_alone_comp_subj_3-1_t6b6_merged.rec -> 20240320_142408_alone_comp_subj_3-1\n",
      "  20240320_142408_alone_comp_subj_3-3_t5b5_merged.rec -> 20240320_142408_alone_comp_subj_3-3\n",
      "  20240320_171038_alone_comp_subj_4-2_t6b6_merged.rec -> 20240320_171038_alone_comp_subj_4-2\n",
      "  20240320_171038_alone_comp_subj_4-3_t5b5_merged.rec -> 20240320_171038_alone_comp_subj_4-3\n",
      "  20240322_120625_alone_comp_subj_3-3_t6b6_merged.rec -> 20240322_120625_alone_comp_subj_3-3\n",
      "  20240322_120625_alone_comp_subj_3-4_t5b5_merged.rec -> 20240322_120625_alone_comp_subj_3-4\n",
      "  20240322_160946_alone_comp_subj_4-3_t6b6_merged.rec -> 20240322_160946_alone_comp_subj_4-3\n",
      "  20240323_122227_alone_comp_subj_5-2_t6b6_merged.rec -> 20240323_122227_alone_comp_subj_5-2\n",
      "  20240323_144517_alone_comp_subj_3-1_t5b5_merged.rec -> 20240323_144517_alone_comp_subj_3-1\n",
      "  20240323_144517_alone_comp_subj_3-4_t6b6_merged.rec -> 20240323_144517_alone_comp_subj_3-4\n",
      "  20240323_165815_alone_comp_subj_4-2_t5b5_merged.rec -> 20240323_165815_alone_comp_subj_4-2\n",
      "\n",
      "Recordings that will still be skipped:\n",
      "  20240317_151922_long_comp_subj_3-1_t6b6_merged.rec -> NO MATCH\n",
      "  20240317_151922_long_comp_subj_3-3_t5b5_merged.rec -> NO MATCH\n",
      "  20240317_172017_long_comp_subj_4-2_t6b6_merged.rec -> NO MATCH\n",
      "  20240317_172017_long_comp_subj_4-3_t5b5_merged.rec -> NO MATCH\n",
      "  20240318_143819_long_comp_subj_3-3_t6b6_merged.rec -> NO MATCH\n",
      "  20240318_143819_long_comp_subj_3-4_t5b5_merged.rec -> NO MATCH\n",
      "  20240318_170933_long_comp_subj_4-3_t6b6_merged.rec -> NO MATCH\n",
      "  20240319_160457_long_comp_subj_4-2_t5b5_merged.rec -> NO MATCH\n",
      "  20240320_114629_long_comp_subj_5-3_t6b6_merged.rec -> NO MATCH\n",
      "  20240321_114851_long_comp_subj_5-3_t5b5_merged.rec -> NO MATCH\n",
      "\n",
      "Processing summary:\n",
      "Total SP recordings: 39\n",
      "Will be processed: 29\n",
      "Will be skipped: 10\n",
      "\n",
      "Testing with first matched recording:\n",
      "SP recording: 20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n",
      "Alt recording: 20230612_101430_standard_comp_to_training_D1_subj_1-3\n",
      "Events in alt recording: ['rewarded', 'omission', 'both_rewarded', 'tie', 'no_comp_win', 'no_comp_lose', 'competitive_win', 'competitive_lose']\n",
      "Testing event: rewarded\n",
      "Number of timestamps for this event: 0\n"
     ]
    }
   ],
   "source": [
    "# Test the mapped functions\n",
    "print(\"Testing the new mapping-based functions:\")\n",
    "\n",
    "# Show which recordings will be processed with the mapping\n",
    "matched_recordings = [rec.name for rec in sp.recordings if recording_mapping.get(rec.name) is not None]\n",
    "unmatched_recordings = [rec.name for rec in sp.recordings if recording_mapping.get(rec.name) is None]\n",
    "\n",
    "print(f\"\\nRecordings that will be processed (with mapping):\")\n",
    "for rec_name in matched_recordings:\n",
    "    alt_name = recording_mapping[rec_name]\n",
    "    print(f\"  {rec_name} -> {alt_name}\")\n",
    "\n",
    "print(f\"\\nRecordings that will still be skipped:\")\n",
    "for rec_name in unmatched_recordings:\n",
    "    print(f\"  {rec_name} -> NO MATCH\")\n",
    "\n",
    "print(f\"\\nProcessing summary:\")\n",
    "print(f\"Total SP recordings: {len(sp.recordings)}\")\n",
    "print(f\"Will be processed: {len(matched_recordings)}\")\n",
    "print(f\"Will be skipped: {len(unmatched_recordings)}\")\n",
    "\n",
    "# Test with a single recording to make sure it works\n",
    "if matched_recordings:\n",
    "    test_rec_name = matched_recordings[0]\n",
    "    test_rec = next(rec for rec in sp.recordings if rec.name == test_rec_name)\n",
    "    alt_rec_name = recording_mapping[test_rec_name]\n",
    "    \n",
    "    print(f\"\\nTesting with first matched recording:\")\n",
    "    print(f\"SP recording: {test_rec_name}\")\n",
    "    print(f\"Alt recording: {alt_rec_name}\")\n",
    "    print(f\"Events in alt recording: {list(alternative_event_dict[alt_rec_name].keys())}\")\n",
    "    \n",
    "    # Test processing one event\n",
    "    first_event = list(alternative_event_dict[alt_rec_name].keys())[0]\n",
    "    print(f\"Testing event: {first_event}\")\n",
    "    print(f\"Number of timestamps for this event: {len(alternative_event_dict[alt_rec_name][first_event])}\")\n",
    "else:\n",
    "    print(\"\\nNo matched recordings found to test with!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c673e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running z-score analysis with alternative event dictionary...\n",
      "\n",
      "Analysis complete!\n",
      "Results shape: (0, 0)\n",
      "Skipped entries: 242\n",
      "No results generated!\n",
      "\n",
      "Skipped entries breakdown:\n",
      "reason\n",
      "no events in alt_event_dict                                                   123\n",
      "no mapping to alt_event_dict                                                   10\n",
      "error: operands could not be broadcast together with shapes (22282,) (2,)       5\n",
      "error: operands could not be broadcast together with shapes (25021,) (2,)       5\n",
      "error: operands could not be broadcast together with shapes (6226,) (2,)        5\n",
      "error: operands could not be broadcast together with shapes (13468,) (2,)       5\n",
      "error: operands could not be broadcast together with shapes (23002,) (2,)       5\n",
      "error: operands could not be broadcast together with shapes (5318,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (1174,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (395,) (2,)         4\n",
      "error: operands could not be broadcast together with shapes (5554,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (7364,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (35203,) (2,)       4\n",
      "error: operands could not be broadcast together with shapes (126,) (2,)         4\n",
      "error: operands could not be broadcast together with shapes (7948,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (7343,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (27862,) (2,)       4\n",
      "error: operands could not be broadcast together with shapes (30028,) (2,)       4\n",
      "error: operands could not be broadcast together with shapes (4247,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (933,) (2,)         3\n",
      "error: operands could not be broadcast together with shapes (42029,) (2,)       3\n",
      "error: operands could not be broadcast together with shapes (2454,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (5778,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (5730,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (2516,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (29859,) (2,)       3\n",
      "error: operands could not be broadcast together with shapes (8155,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (19485,) (2,)       3\n",
      "error: operands could not be broadcast together with shapes (8340,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (12603,) (2,)       3\n",
      "error: operands could not be broadcast together with shapes (6276,) (2,)        3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Analysis complete!\n",
      "Results shape: (0, 0)\n",
      "Skipped entries: 242\n",
      "No results generated!\n",
      "\n",
      "Skipped entries breakdown:\n",
      "reason\n",
      "no events in alt_event_dict                                                   123\n",
      "no mapping to alt_event_dict                                                   10\n",
      "error: operands could not be broadcast together with shapes (22282,) (2,)       5\n",
      "error: operands could not be broadcast together with shapes (25021,) (2,)       5\n",
      "error: operands could not be broadcast together with shapes (6226,) (2,)        5\n",
      "error: operands could not be broadcast together with shapes (13468,) (2,)       5\n",
      "error: operands could not be broadcast together with shapes (23002,) (2,)       5\n",
      "error: operands could not be broadcast together with shapes (5318,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (1174,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (395,) (2,)         4\n",
      "error: operands could not be broadcast together with shapes (5554,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (7364,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (35203,) (2,)       4\n",
      "error: operands could not be broadcast together with shapes (126,) (2,)         4\n",
      "error: operands could not be broadcast together with shapes (7948,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (7343,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (27862,) (2,)       4\n",
      "error: operands could not be broadcast together with shapes (30028,) (2,)       4\n",
      "error: operands could not be broadcast together with shapes (4247,) (2,)        4\n",
      "error: operands could not be broadcast together with shapes (933,) (2,)         3\n",
      "error: operands could not be broadcast together with shapes (42029,) (2,)       3\n",
      "error: operands could not be broadcast together with shapes (2454,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (5778,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (5730,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (2516,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (29859,) (2,)       3\n",
      "error: operands could not be broadcast together with shapes (8155,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (19485,) (2,)       3\n",
      "error: operands could not be broadcast together with shapes (8340,) (2,)        3\n",
      "error: operands could not be broadcast together with shapes (12603,) (2,)       3\n",
      "error: operands could not be broadcast together with shapes (6276,) (2,)        3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis using the alternative event dictionary\n",
    "print(\"Running z-score analysis with alternative event dictionary...\")\n",
    "\n",
    "# Run the analysis\n",
    "big_df, skipped_df = run_all_recordings_all_events_mapped(\n",
    "    sp, \n",
    "    alternative_event_dict, \n",
    "    recording_mapping, \n",
    "    pre_window=10, \n",
    "    SD=1.65, \n",
    "    verbose=False  # Set to True if you want detailed output\n",
    ")\n",
    "\n",
    "print(f\"\\nAnalysis complete!\")\n",
    "print(f\"Results shape: {big_df.shape}\")\n",
    "print(f\"Skipped entries: {skipped_df.shape[0]}\")\n",
    "\n",
    "if not big_df.empty:\n",
    "    print(f\"\\nFirst few rows of results:\")\n",
    "    print(big_df.head())\n",
    "    \n",
    "    print(f\"\\nUnique recordings processed: {big_df['Recording'].nunique()}\")\n",
    "    print(f\"Unique events analyzed: {big_df['Event name'].nunique()}\")\n",
    "    print(f\"Event types: {sorted(big_df['Event name'].unique())}\")\n",
    "else:\n",
    "    print(\"No results generated!\")\n",
    "\n",
    "if not skipped_df.empty:\n",
    "    print(f\"\\nSkipped entries breakdown:\")\n",
    "    print(skipped_df['reason'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95dd0bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results to save.\n"
     ]
    }
   ],
   "source": [
    "# Save the results if you want to\n",
    "if not big_df.empty:\n",
    "    # Uncomment the lines below to save the results\n",
    "    # output_path = r'C:\\Users\\thoma\\Code\\ResearchCode\\diff_fam_social_memory_ephys\\other_peoples_sutff\\Thomas\\reward_comp\\testing_eventdict_creation\\zscore_results_alternative_events.csv'\n",
    "    # big_df.to_csv(output_path, index=False)\n",
    "    # print(f\"Results saved to: {output_path}\")\n",
    "    \n",
    "    # Show some summary statistics\n",
    "    print(\"Summary statistics:\")\n",
    "    print(f\"Total units analyzed: {big_df['Unit number'].nunique()}\")\n",
    "    print(f\"Total recordings: {big_df['Recording'].nunique()}\")\n",
    "    print(f\"Total events: {big_df['Event name'].nunique()}\")\n",
    "    \n",
    "    # Show significance distribution\n",
    "    print(f\"\\nSignificance distribution:\")\n",
    "    print(big_df['sig'].value_counts())\n",
    "    \n",
    "    # Show z-score statistics\n",
    "    print(f\"\\nZ-score statistics:\")\n",
    "    print(big_df['Event Z-Score'].describe())\n",
    "else:\n",
    "    print(\"No results to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2e6a7",
   "metadata": {},
   "source": [
    "### Simple Direct Approach - Use Alt Event Dict Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zscore_direct_alt_events_fixed(sp, alt_event_dict, pre_window=10, SD=1.65, verbose=False):\n",
    "    \"\"\"\n",
    "    Run z-score analysis using ONLY the alternative event dictionary.\n",
    "    No mapping needed - directly uses alt_event_dict structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - sp: SpikeCollection object (for spike data only)\n",
    "    - alt_event_dict: Alternative event dictionary with structure {recording_name: {event_name: [timestamps]}}\n",
    "    - pre_window: Duration in seconds before event for baseline\n",
    "    - SD: Standard deviation threshold for significance\n",
    "    - verbose: Print debug info\n",
    "    \"\"\"\n",
    "    \n",
    "    all_results = []\n",
    "    skipped = []\n",
    "    \n",
    "    # Get all unique event names from alt_event_dict\n",
    "    all_event_names = set()\n",
    "    for rec_events in alt_event_dict.values():\n",
    "        all_event_names.update(rec_events.keys())\n",
    "    all_event_names = sorted(all_event_names)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Events found in alt_event_dict: {all_event_names}\")\n",
    "    \n",
    "    # Process each recording in alt_event_dict\n",
    "    for alt_rec_name, events_dict in alt_event_dict.items():\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing recording: {alt_rec_name}\")\n",
    "        \n",
    "        # Find matching spike collection recording (by substring matching)\n",
    "        spike_recording = None\n",
    "        for sp_rec in sp.recordings:\n",
    "            if alt_rec_name in sp_rec.name:  # Simple substring match\n",
    "                spike_recording = sp_rec\n",
    "                break\n",
    "        \n",
    "        if spike_recording is None:\n",
    "            skipped.append((alt_rec_name, \"ALL_EVENTS\", \"no matching spike recording\"))\n",
    "            if verbose:\n",
    "                print(f\"  No matching spike recording found for {alt_rec_name}\")\n",
    "            continue\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Matched to spike recording: {spike_recording.name}\")\n",
    "        \n",
    "        # Get good units\n",
    "        units = getattr(spike_recording, \"good_units\", None)\n",
    "        if units is None:\n",
    "            units = [unit_id for unit_id, label in spike_recording.labels_dict.items() if label == \"good\"]\n",
    "        \n",
    "        if not units:\n",
    "            skipped.append((alt_rec_name, \"ALL_EVENTS\", \"no good units\"))\n",
    "            continue\n",
    "        \n",
    "        # Step 1: Pool ALL baselines across ALL events for each unit\n",
    "        global_baseline_counts = {unit_id: [] for unit_id in units}\n",
    "        \n",
    "        for event_name, event_windows in events_dict.items():\n",
    "            if len(event_windows) == 0:\n",
    "                continue\n",
    "                \n",
    "            for unit_id in units:\n",
    "                spikes = spike_recording.unit_timestamps[unit_id]\n",
    "                spikes_ms = spikes * (1000 / spike_recording.sampling_rate)\n",
    "                \n",
    "                # For each event window, calculate baseline before the start\n",
    "                for window in event_windows:\n",
    "                    if len(window) >= 2:  # Should be [start, end]\n",
    "                        event_start = window[0]\n",
    "                        start_baseline = event_start - int(pre_window * 1000)\n",
    "                        end_baseline = event_start\n",
    "                        baseline_count = np.sum((spikes_ms >= start_baseline) & (spikes_ms < end_baseline))\n",
    "                        global_baseline_counts[unit_id].append(baseline_count)\n",
    "        \n",
    "        # Step 2: Calculate global baseline stats per unit\n",
    "        baseline_mean = {u: np.mean(counts) if counts else 0 for u, counts in global_baseline_counts.items()}\n",
    "        baseline_sd = {u: np.std(counts) if counts else 0 for u, counts in global_baseline_counts.items()}\n",
    "        \n",
    "        # Step 3: Calculate z-scores for each event\n",
    "        for event_name in all_event_names:\n",
    "            if event_name not in events_dict or len(events_dict[event_name]) == 0:\n",
    "                skipped.append((alt_rec_name, event_name, \"no events\"))\n",
    "                continue\n",
    "            \n",
    "            event_timestamps = events_dict[event_name]\n",
    "            \n",
    "            for unit_id in units:\n",
    "                spikes = spike_recording.unit_timestamps[unit_id]\n",
    "                spikes_ms = spikes * (1000 / spike_recording.sampling_rate)\n",
    "                \n",
    "                # Count spikes for each event occurrence (1 second windows)\n",
    "                event_counts = []\n",
    "                for timestamp in event_timestamps:\n",
    "                    start_event = timestamp\n",
    "                    end_event = timestamp + 1000  # 1 second window\n",
    "                    event_count = np.sum((spikes_ms >= start_event) & (spikes_ms < end_event))\n",
    "                    event_counts.append(event_count)\n",
    "                \n",
    "                # Calculate stats\n",
    "                ev_mean = np.mean(event_counts) if event_counts else 0\n",
    "                b_mean = baseline_mean[unit_id]\n",
    "                b_sd = baseline_sd[unit_id]\n",
    "                \n",
    "                # Z-score calculation\n",
    "                zscore = np.nan if b_sd == 0 else (ev_mean - b_mean) / b_sd\n",
    "                \n",
    "                # Significance\n",
    "                if np.isnan(zscore):\n",
    "                    sig = \"not sig\"\n",
    "                elif zscore > SD:\n",
    "                    sig = \"increase\"\n",
    "                elif zscore < -SD:\n",
    "                    sig = \"decrease\"\n",
    "                else:\n",
    "                    sig = \"not sig\"\n",
    "                \n",
    "                all_results.append({\n",
    "                    \"Alt_Recording\": alt_rec_name,\n",
    "                    \"SP_Recording\": spike_recording.name,\n",
    "                    \"Event name\": event_name,\n",
    "                    \"Unit number\": unit_id,\n",
    "                    \"Global Pre-event M\": b_mean,\n",
    "                    \"Global Pre-event SD\": b_sd,\n",
    "                    \"Event M\": ev_mean,\n",
    "                    \"Event Z-Score\": zscore,\n",
    "                    \"sig\": sig,\n",
    "                    \"Event count\": len(event_timestamps)\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(all_results) if all_results else pd.DataFrame()\n",
    "    skipped_df = pd.DataFrame(skipped, columns=[\"Recording\", \"Event name\", \"reason\"]) if skipped else pd.DataFrame()\n",
    "    \n",
    "    return results_df, skipped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "270f15ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running z-score analysis directly with alternative event dictionary...\n",
      "This bypasses all spike collection events and uses ONLY your alt_event_dict\n",
      "Events found in alt_event_dict: ['both_rewarded', 'competitive_lose', 'competitive_win', 'no_comp_lose', 'no_comp_win', 'omission', 'rewarded', 'tie']\n",
      "\n",
      "Processing recording: 20230612_101430_standard_comp_to_training_D1_subj_1-3\n",
      "  Matched to spike recording: 20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (395,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis bypasses all spike collection events and uses ONLY your alt_event_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Run the simple direct analysis\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m results_df, skipped_df \u001b[38;5;241m=\u001b[39m \u001b[43mrun_zscore_direct_alt_events\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43malternative_event_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.65\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to False to reduce output\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDIRECT ANALYSIS COMPLETE!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 71\u001b[0m, in \u001b[0;36mrun_zscore_direct_alt_events\u001b[1;34m(sp, alt_event_dict, pre_window, SD, verbose)\u001b[0m\n\u001b[0;32m     69\u001b[0m             start_baseline \u001b[38;5;241m=\u001b[39m timestamp \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mint\u001b[39m(pre_window \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     70\u001b[0m             end_baseline \u001b[38;5;241m=\u001b[39m timestamp\n\u001b[1;32m---> 71\u001b[0m             baseline_count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((\u001b[43mspikes_ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_baseline\u001b[49m) \u001b[38;5;241m&\u001b[39m (spikes_ms \u001b[38;5;241m<\u001b[39m end_baseline))\n\u001b[0;32m     72\u001b[0m             global_baseline_counts[unit_id]\u001b[38;5;241m.\u001b[39mappend(baseline_count)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Step 2: Calculate global baseline stats per unit\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (395,) (2,) "
     ]
    }
   ],
   "source": [
    "# Test the direct approach - no mapping needed!\n",
    "print(\"Running z-score analysis directly with alternative event dictionary...\")\n",
    "print(\"This bypasses all spike collection events and uses ONLY your alt_event_dict\")\n",
    "\n",
    "# Run the simple direct analysis\n",
    "results_df, skipped_df = run_zscore_direct_alt_events(\n",
    "    sp, \n",
    "    alternative_event_dict, \n",
    "    pre_window=10, \n",
    "    SD=1.65, \n",
    "    verbose=True  # Set to False to reduce output\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"DIRECT ANALYSIS COMPLETE!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Results shape: {results_df.shape}\")\n",
    "print(f\"Skipped entries: {skipped_df.shape[0]}\")\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(results_df.head())\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"- Recordings processed: {results_df['Alt_Recording'].nunique()}\")\n",
    "    print(f\"- Event types: {results_df['Event name'].nunique()}\")\n",
    "    print(f\"- Total units: {results_df['Unit number'].nunique()}\")\n",
    "    print(f\"- Event types found: {sorted(results_df['Event name'].unique())}\")\n",
    "    \n",
    "    print(f\"\\nSignificance counts:\")\n",
    "    print(results_df['sig'].value_counts())\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No results generated!\")\n",
    "\n",
    "if not skipped_df.empty:\n",
    "    print(f\"\\nSkipped breakdown:\")\n",
    "    print(skipped_df['reason'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce7556d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event structure analysis:\n",
      "\n",
      "Event: rewarded\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (0,)\n",
      "\n",
      "Event: omission\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (0,)\n",
      "\n",
      "Event: both_rewarded\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (0,)\n",
      "\n",
      "Event: tie\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (0,)\n",
      "\n",
      "Event: no_comp_win\n",
      "  Type: <class 'numpy.ndarray'>\n",
      "  Shape: (5, 2)\n",
      "  First few values: [[169111 179111]\n",
      " [479111 489111]\n",
      " [804110 814110]]\n",
      "  Value types: <class 'numpy.ndarray'>\n",
      "\n",
      "Looking for non-empty events:\n",
      "Non-empty event found: no_comp_win\n",
      "  Shape: (5, 2)\n",
      "  First value: [169111 179111]\n",
      "  Type of first value: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# First, let's inspect the structure of your alternative event dictionary\n",
    "test_rec_name = \"20230612_101430_standard_comp_to_training_D1_subj_1-3\"\n",
    "if test_rec_name in alternative_event_dict:\n",
    "    events = alternative_event_dict[test_rec_name]\n",
    "    print(\"Event structure analysis:\")\n",
    "    for event_name, event_data in events.items():\n",
    "        print(f\"\\nEvent: {event_name}\")\n",
    "        print(f\"  Type: {type(event_data)}\")\n",
    "        print(f\"  Shape: {np.array(event_data).shape}\")\n",
    "        if len(event_data) > 0:\n",
    "            print(f\"  First few values: {event_data[:3]}\")\n",
    "            print(f\"  Value types: {type(event_data[0]) if len(event_data) > 0 else 'N/A'}\")\n",
    "            break  # Found a non-empty event, stop here\n",
    "    \n",
    "    # Find a non-empty event\n",
    "    print(\"\\nLooking for non-empty events:\")\n",
    "    for event_name, event_data in events.items():\n",
    "        if len(event_data) > 0:\n",
    "            print(f\"Non-empty event found: {event_name}\")\n",
    "            print(f\"  Shape: {np.array(event_data).shape}\")\n",
    "            print(f\"  First value: {event_data[0]}\")\n",
    "            print(f\"  Type of first value: {type(event_data[0])}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3661f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zscore_direct_alt_events_fixed(sp, alt_event_dict, pre_window=10, SD=1.65, verbose=False):\n",
    "    \"\"\"\n",
    "    Run z-score analysis using ONLY the alternative event dictionary.\n",
    "    FIXED: Handles event windows [start, end] instead of single timestamps.\n",
    "    \n",
    "    Parameters:\n",
    "    - sp: SpikeCollection object (for spike data only)\n",
    "    - alt_event_dict: Alternative event dictionary with structure {recording_name: {event_name: [[start, end], ...]}}\n",
    "    - pre_window: Duration in seconds before event for baseline\n",
    "    - SD: Standard deviation threshold for significance\n",
    "    - verbose: Print debug info\n",
    "    \"\"\"\n",
    "    \n",
    "    all_results = []\n",
    "    skipped = []\n",
    "    \n",
    "    # Get all unique event names from alt_event_dict\n",
    "    all_event_names = set()\n",
    "    for rec_events in alt_event_dict.values():\n",
    "        all_event_names.update(rec_events.keys())\n",
    "    all_event_names = sorted(all_event_names)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Events found in alt_event_dict: {all_event_names}\")\n",
    "    \n",
    "    # Process each recording in alt_event_dict\n",
    "    for alt_rec_name, events_dict in alt_event_dict.items():\n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing recording: {alt_rec_name}\")\n",
    "        \n",
    "        # Find matching spike collection recording (by substring matching)\n",
    "        spike_recording = None\n",
    "        for sp_rec in sp.recordings:\n",
    "            if alt_rec_name in sp_rec.name:  # Simple substring match\n",
    "                spike_recording = sp_rec\n",
    "                break\n",
    "        \n",
    "        if spike_recording is None:\n",
    "            skipped.append((alt_rec_name, \"ALL_EVENTS\", \"no matching spike recording\"))\n",
    "            if verbose:\n",
    "                print(f\"  No matching spike recording found for {alt_rec_name}\")\n",
    "            continue\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Matched to spike recording: {spike_recording.name}\")\n",
    "        \n",
    "        # Get good units\n",
    "        units = getattr(spike_recording, \"good_units\", None)\n",
    "        if units is None:\n",
    "            units = [unit_id for unit_id, label in spike_recording.labels_dict.items() if label == \"good\"]\n",
    "        \n",
    "        if not units:\n",
    "            skipped.append((alt_rec_name, \"ALL_EVENTS\", \"no good units\"))\n",
    "            continue\n",
    "        \n",
    "        # Step 1: Pool ALL baselines across ALL events for each unit\n",
    "        global_baseline_counts = {unit_id: [] for unit_id in units}\n",
    "        \n",
    "        for event_name, event_windows in events_dict.items():\n",
    "            if len(event_windows) == 0:\n",
    "                continue\n",
    "                \n",
    "            for unit_id in units:\n",
    "                spikes = spike_recording.unit_timestamps[unit_id]\n",
    "                spikes_ms = spikes * (1000 / spike_recording.sampling_rate)\n",
    "                \n",
    "                # For each event window, calculate baseline before the start\n",
    "                for window in event_windows:\n",
    "                    if len(window) >= 2:  # Should be [start, end]\n",
    "                        event_start = window[0]\n",
    "                        start_baseline = event_start - int(pre_window * 1000)\n",
    "                        end_baseline = event_start\n",
    "                        baseline_count = np.sum((spikes_ms >= start_baseline) & (spikes_ms < end_baseline))\n",
    "                        global_baseline_counts[unit_id].append(baseline_count)\n",
    "        \n",
    "        # Step 2: Calculate global baseline stats per unit\n",
    "        baseline_mean = {u: np.mean(counts) if counts else 0 for u, counts in global_baseline_counts.items()}\n",
    "        baseline_sd = {u: np.std(counts) if counts else 0 for u, counts in global_baseline_counts.items()}\n",
    "        \n",
    "        # Step 3: Calculate z-scores for each event\n",
    "        for event_name in all_event_names:\n",
    "            if event_name not in events_dict or len(events_dict[event_name]) == 0:\n",
    "                skipped.append((alt_rec_name, event_name, \"no events\"))\n",
    "                continue\n",
    "            \n",
    "            event_windows = events_dict[event_name]\n",
    "            \n",
    "            for unit_id in units:\n",
    "                spikes = spike_recording.unit_timestamps[unit_id]\n",
    "                spikes_ms = spikes * (1000 / spike_recording.sampling_rate)\n",
    "                \n",
    "                # Count spikes for each event window\n",
    "                event_counts = []\n",
    "                for window in event_windows:\n",
    "                    if len(window) >= 2:\n",
    "                        start_event = window[0]\n",
    "                        end_event = window[1]\n",
    "                        event_count = np.sum((spikes_ms >= start_event) & (spikes_ms < end_event))\n",
    "                        event_counts.append(event_count)\n",
    "                \n",
    "                # Calculate stats\n",
    "                ev_mean = np.mean(event_counts) if event_counts else 0\n",
    "                b_mean = baseline_mean[unit_id]\n",
    "                b_sd = baseline_sd[unit_id]\n",
    "                \n",
    "                # Z-score calculation\n",
    "                zscore = np.nan if b_sd == 0 else (ev_mean - b_mean) / b_sd\n",
    "                \n",
    "                # Significance\n",
    "                if np.isnan(zscore):\n",
    "                    sig = \"not sig\"\n",
    "                elif zscore > SD:\n",
    "                    sig = \"increase\"\n",
    "                elif zscore < -SD:\n",
    "                    sig = \"decrease\"\n",
    "                else:\n",
    "                    sig = \"not sig\"\n",
    "                \n",
    "                all_results.append({\n",
    "                    \"Alt_Recording\": alt_rec_name,\n",
    "                    \"SP_Recording\": spike_recording.name,\n",
    "                    \"Event name\": event_name,\n",
    "                    \"Unit number\": unit_id,\n",
    "                    \"Global Pre-event M\": b_mean,\n",
    "                    \"Global Pre-event SD\": b_sd,\n",
    "                    \"Event M\": ev_mean,\n",
    "                    \"Event Z-Score\": zscore,\n",
    "                    \"sig\": sig,\n",
    "                    \"Event count\": len(event_windows)\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(all_results) if all_results else pd.DataFrame()\n",
    "    skipped_df = pd.DataFrame(skipped, columns=[\"Recording\", \"Event name\", \"reason\"]) if skipped else pd.DataFrame()\n",
    "    \n",
    "    return results_df, skipped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a72dfffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Running FIXED z-score analysis with alternative event dictionary...\n",
      "Now correctly handles [start, end] time windows!\n",
      "\n",
      "==================================================\n",
      "âœ… ANALYSIS COMPLETE!\n",
      "==================================================\n",
      "Results shape: (1908, 10)\n",
      "Skipped entries: 134\n",
      "\n",
      "ðŸ“Š SUCCESS! Generated 1908 results\n",
      "\n",
      "First few rows:\n",
      "                                           Alt_Recording  \\\n",
      "0  20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
      "1  20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
      "2  20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
      "3  20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
      "4  20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
      "\n",
      "                                                                  SP_Recording  \\\n",
      "0  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
      "1  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
      "2  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
      "3  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
      "4  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
      "\n",
      "         Event name Unit number  Global Pre-event M  Global Pre-event SD  \\\n",
      "0  competitive_lose         104                1.30             2.776689   \n",
      "1  competitive_lose          11                8.10             8.086408   \n",
      "2  competitive_lose         122                0.40             0.583095   \n",
      "3  competitive_lose         125                6.35             4.077683   \n",
      "4  competitive_lose         126               16.50            25.482347   \n",
      "\n",
      "   Event M  Event Z-Score      sig  Event count  \n",
      "0      1.0      -0.108042  not sig            1  \n",
      "1      0.0      -1.001681  not sig            1  \n",
      "2      0.0      -0.685994  not sig            1  \n",
      "3      4.0      -0.576308  not sig            1  \n",
      "4      4.0      -0.490536  not sig            1  \n",
      "\n",
      "ðŸ“ˆ Summary:\n",
      "- Recordings processed: 29\n",
      "- Event types: 5\n",
      "- Total units: 231\n",
      "- Event types found: ['competitive_lose', 'competitive_win', 'no_comp_lose', 'no_comp_win', 'tie']\n",
      "\n",
      "ðŸŽ¯ Significance counts:\n",
      "  not sig: 1774\n",
      "  increase: 118\n",
      "  decrease: 16\n",
      "\n",
      "ðŸ“Š Z-score statistics:\n",
      "count    1905.000000\n",
      "mean        0.160580\n",
      "std         1.189328\n",
      "min        -3.370750\n",
      "25%        -0.439555\n",
      "50%        -0.045601\n",
      "75%         0.506898\n",
      "max        17.884284\n",
      "Name: Event Z-Score, dtype: float64\n",
      "\n",
      "âš ï¸  Skipped breakdown:\n",
      "reason\n",
      "no events                      123\n",
      "no matching spike recording     11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Test the FIXED direct approach\n",
    "print(\"ðŸš€ Running FIXED z-score analysis with alternative event dictionary...\")\n",
    "print(\"Now correctly handles [start, end] time windows!\")\n",
    "\n",
    "# Run the corrected analysis\n",
    "results_df, skipped_df = run_zscore_direct_alt_events_fixed(\n",
    "    sp, \n",
    "    alternative_event_dict, \n",
    "    pre_window=10, \n",
    "    SD=1.65, \n",
    "    verbose=False  # Change to True for detailed output\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"âœ… ANALYSIS COMPLETE!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Results shape: {results_df.shape}\")\n",
    "print(f\"Skipped entries: {skipped_df.shape[0]}\")\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(f\"\\nðŸ“Š SUCCESS! Generated {results_df.shape[0]} results\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(results_df.head())\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Summary:\")\n",
    "    print(f\"- Recordings processed: {results_df['Alt_Recording'].nunique()}\")\n",
    "    print(f\"- Event types: {results_df['Event name'].nunique()}\")\n",
    "    print(f\"- Total units: {results_df['Unit number'].nunique()}\")\n",
    "    print(f\"- Event types found: {sorted(results_df['Event name'].unique())}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Significance counts:\")\n",
    "    sig_counts = results_df['sig'].value_counts()\n",
    "    for sig_type, count in sig_counts.items():\n",
    "        print(f\"  {sig_type}: {count}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Z-score statistics:\")\n",
    "    print(results_df['Event Z-Score'].describe())\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No results generated!\")\n",
    "\n",
    "if not skipped_df.empty:\n",
    "    print(f\"\\nâš ï¸  Skipped breakdown:\")\n",
    "    print(skipped_df['reason'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd835b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Alt_Recording",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "SP_Recording",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Event name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Unit number",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Global Pre-event M",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Global Pre-event SD",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Event M",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Event Z-Score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sig",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Event count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "a53cfa6c-2406-4c84-b53f-d29a1f1b4c86",
       "rows": [
        [
         "0",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec",
         "competitive_lose",
         "104",
         "1.3",
         "2.776688675382964",
         "1.0",
         "-0.108042360909843",
         "not sig",
         "1"
        ],
        [
         "1",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec",
         "competitive_lose",
         "11",
         "8.1",
         "8.086408349817612",
         "0.0",
         "-1.001680801858429",
         "not sig",
         "1"
        ],
        [
         "2",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec",
         "competitive_lose",
         "122",
         "0.4",
         "0.5830951894845301",
         "0.0",
         "-0.6859943405700353",
         "not sig",
         "1"
        ],
        [
         "3",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec",
         "competitive_lose",
         "125",
         "6.35",
         "4.0776831657204555",
         "4.0",
         "-0.5763076493425392",
         "not sig",
         "1"
        ],
        [
         "4",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec",
         "competitive_lose",
         "126",
         "16.5",
         "25.482346830698308",
         "4.0",
         "-0.4905356670266094",
         "not sig",
         "1"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alt_Recording</th>\n",
       "      <th>SP_Recording</th>\n",
       "      <th>Event name</th>\n",
       "      <th>Unit number</th>\n",
       "      <th>Global Pre-event M</th>\n",
       "      <th>Global Pre-event SD</th>\n",
       "      <th>Event M</th>\n",
       "      <th>Event Z-Score</th>\n",
       "      <th>sig</th>\n",
       "      <th>Event count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec</td>\n",
       "      <td>competitive_lose</td>\n",
       "      <td>104</td>\n",
       "      <td>1.30</td>\n",
       "      <td>2.776689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.108042</td>\n",
       "      <td>not sig</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec</td>\n",
       "      <td>competitive_lose</td>\n",
       "      <td>11</td>\n",
       "      <td>8.10</td>\n",
       "      <td>8.086408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.001681</td>\n",
       "      <td>not sig</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec</td>\n",
       "      <td>competitive_lose</td>\n",
       "      <td>122</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.583095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.685994</td>\n",
       "      <td>not sig</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec</td>\n",
       "      <td>competitive_lose</td>\n",
       "      <td>125</td>\n",
       "      <td>6.35</td>\n",
       "      <td>4.077683</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.576308</td>\n",
       "      <td>not sig</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec</td>\n",
       "      <td>competitive_lose</td>\n",
       "      <td>126</td>\n",
       "      <td>16.50</td>\n",
       "      <td>25.482347</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.490536</td>\n",
       "      <td>not sig</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Alt_Recording  \\\n",
       "0  20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
       "1  20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
       "2  20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
       "3  20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
       "4  20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
       "\n",
       "                                                                  SP_Recording  \\\n",
       "0  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
       "1  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
       "2  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
       "3  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
       "4  20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
       "\n",
       "         Event name Unit number  Global Pre-event M  Global Pre-event SD  \\\n",
       "0  competitive_lose         104                1.30             2.776689   \n",
       "1  competitive_lose          11                8.10             8.086408   \n",
       "2  competitive_lose         122                0.40             0.583095   \n",
       "3  competitive_lose         125                6.35             4.077683   \n",
       "4  competitive_lose         126               16.50            25.482347   \n",
       "\n",
       "   Event M  Event Z-Score      sig  Event count  \n",
       "0      1.0      -0.108042  not sig            1  \n",
       "1      0.0      -1.001681  not sig            1  \n",
       "2      0.0      -0.685994  not sig            1  \n",
       "3      4.0      -0.576308  not sig            1  \n",
       "4      4.0      -0.490536  not sig            1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7259735e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['competitive_lose', 'competitive_win', 'no_comp_lose',\n",
       "       'no_comp_win', 'tie'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['Event name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b5125ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Recording",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Event name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "reason",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b02481a4-79d6-4a6a-8a51-26effe8c6029",
       "rows": [
        [
         "0",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "both_rewarded",
         "no events"
        ],
        [
         "1",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "omission",
         "no events"
        ],
        [
         "2",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "rewarded",
         "no events"
        ],
        [
         "3",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "tie",
         "no events"
        ],
        [
         "4",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4",
         "both_rewarded",
         "no events"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recording</th>\n",
       "      <th>Event name</th>\n",
       "      <th>reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>both_rewarded</td>\n",
       "      <td>no events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>omission</td>\n",
       "      <td>no events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>rewarded</td>\n",
       "      <td>no events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>tie</td>\n",
       "      <td>no events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4</td>\n",
       "      <td>both_rewarded</td>\n",
       "      <td>no events</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Recording     Event name  \\\n",
       "0  20230612_101430_standard_comp_to_training_D1_subj_1-3  both_rewarded   \n",
       "1  20230612_101430_standard_comp_to_training_D1_subj_1-3       omission   \n",
       "2  20230612_101430_standard_comp_to_training_D1_subj_1-3       rewarded   \n",
       "3  20230612_101430_standard_comp_to_training_D1_subj_1-3            tie   \n",
       "4  20230612_101430_standard_comp_to_training_D1_subj_1-4  both_rewarded   \n",
       "\n",
       "      reason  \n",
       "0  no events  \n",
       "1  no events  \n",
       "2  no events  \n",
       "3  no events  \n",
       "4  no events  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9295fe",
   "metadata": {},
   "source": [
    "### Event Name Mapping for Analysis Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3856064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event name mapping:\n",
      "  no_comp_win -> low_comp_win\n",
      "  no_comp_lose -> low_comp_lose\n",
      "  competitive_win -> high_comp_win\n",
      "  competitive_lose -> high_comp_lose\n",
      "  rewarded -> alone_rewarded\n",
      "  both_rewarded -> win\n",
      "  omission -> SKIP\n",
      "  tie -> SKIP\n",
      "\n",
      "Events in alt_event_dict:\n",
      "  ['both_rewarded', 'competitive_lose', 'competitive_win', 'no_comp_lose', 'no_comp_win', 'omission', 'rewarded', 'tie']\n",
      "\n",
      "Your analysis expects these events:\n",
      "  ['alone_rewarded', 'alone_rewarded_baseline', 'high_comp', 'high_comp_lose', 'high_comp_lose_baseline', 'high_comp_win', 'high_comp_win_baseline', 'lose', 'low_comp', 'low_comp_lose', 'low_comp_lose_baseline', 'low_comp_win', 'low_comp_win_baseline', 'overall_pretone', 'win']\n"
     ]
    }
   ],
   "source": [
    "# Create mapping from your analysis event names to alt_event_dict names\n",
    "def create_event_name_mapping():\n",
    "    \"\"\"\n",
    "    Map analysis event names to alternative event dictionary names\n",
    "    \"\"\"\n",
    "    # Your analysis events: ['alone_rewarded', 'alone_rewarded_baseline', 'high_comp', 'high_comp_lose', \n",
    "    # 'high_comp_lose_baseline', 'high_comp_win', 'high_comp_win_baseline', 'lose', 'low_comp', \n",
    "    # 'low_comp_lose', 'low_comp_lose_baseline', 'low_comp_win', 'low_comp_win_baseline', 'overall_pretone', 'win']\n",
    "    \n",
    "    # Alt event dict events: ['both_rewarded', 'competitive_lose', 'competitive_win', 'no_comp_lose', 'no_comp_win', 'omission', 'rewarded', 'tie']\n",
    "    \n",
    "    event_mapping = {\n",
    "        # Map no_comp events to low_comp equivalents\n",
    "        'no_comp_win': 'low_comp_win',\n",
    "        'no_comp_lose': 'low_comp_lose', \n",
    "        \n",
    "        # Map competitive events to high_comp equivalents  \n",
    "        'competitive_win': 'high_comp_win',\n",
    "        'competitive_lose': 'high_comp_lose',\n",
    "        \n",
    "        # Map rewarded events\n",
    "        'rewarded': 'alone_rewarded',\n",
    "        'both_rewarded': 'win',  # or could be 'high_comp' depending on your preference\n",
    "        \n",
    "        # Events that don't have clear mappings - keep as is or skip\n",
    "        'omission': None,  # Skip this one\n",
    "        'tie': None,       # Skip this one\n",
    "    }\n",
    "    \n",
    "    return event_mapping\n",
    "\n",
    "# Create the mapping\n",
    "event_mapping = create_event_name_mapping()\n",
    "\n",
    "print(\"Event name mapping:\")\n",
    "for alt_name, analysis_name in event_mapping.items():\n",
    "    if analysis_name:\n",
    "        print(f\"  {alt_name} -> {analysis_name}\")\n",
    "    else:\n",
    "        print(f\"  {alt_name} -> SKIP\")\n",
    "\n",
    "# Show what events are available in alt_event_dict\n",
    "print(f\"\\nEvents in alt_event_dict:\")\n",
    "alt_events = set()\n",
    "for rec_events in alternative_event_dict.values():\n",
    "    alt_events.update(rec_events.keys())\n",
    "print(f\"  {sorted(alt_events)}\")\n",
    "\n",
    "print(f\"\\nYour analysis expects these events:\")\n",
    "analysis_events = ['alone_rewarded', 'alone_rewarded_baseline', 'high_comp', 'high_comp_lose', \n",
    "                  'high_comp_lose_baseline', 'high_comp_win', 'high_comp_win_baseline', 'lose', 'low_comp', \n",
    "                  'low_comp_lose', 'low_comp_lose_baseline', 'low_comp_win', 'low_comp_win_baseline', 'overall_pretone', 'win']\n",
    "print(f\"  {analysis_events}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a79c990e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying event name mapping to results...\n",
      "Original DataFrame shape: (1908, 10)\n",
      "Original events: ['competitive_lose', 'competitive_win', 'no_comp_lose', 'no_comp_win', 'tie']\n",
      "Mapped DataFrame shape: (1908, 10)\n",
      "Mapped events: ['high_comp_lose', 'high_comp_win', 'low_comp_lose', 'low_comp_win', 'tie']\n",
      "Rows skipped (unmapped events): 0\n",
      "\n",
      "ðŸ“Š Mapping Summary:\n",
      "- Original results: 1908 rows\n",
      "- Mapped results: 1908 rows\n",
      "- Events after mapping: ['high_comp_lose', 'high_comp_win', 'low_comp_lose', 'low_comp_win', 'tie']\n",
      "\n",
      "Sample of mapped results:\n",
      "                                           Alt_Recording      Event name  \\\n",
      "0  20230612_101430_standard_comp_to_training_D1_subj_1-3  high_comp_lose   \n",
      "1  20230612_101430_standard_comp_to_training_D1_subj_1-3  high_comp_lose   \n",
      "2  20230612_101430_standard_comp_to_training_D1_subj_1-3  high_comp_lose   \n",
      "3  20230612_101430_standard_comp_to_training_D1_subj_1-3  high_comp_lose   \n",
      "4  20230612_101430_standard_comp_to_training_D1_subj_1-3  high_comp_lose   \n",
      "5  20230612_101430_standard_comp_to_training_D1_subj_1-3  high_comp_lose   \n",
      "6  20230612_101430_standard_comp_to_training_D1_subj_1-3  high_comp_lose   \n",
      "7  20230612_101430_standard_comp_to_training_D1_subj_1-3  high_comp_lose   \n",
      "8  20230612_101430_standard_comp_to_training_D1_subj_1-3  high_comp_lose   \n",
      "9  20230612_101430_standard_comp_to_training_D1_subj_1-3  high_comp_lose   \n",
      "\n",
      "  Unit number  Event Z-Score      sig  \n",
      "0         104      -0.108042  not sig  \n",
      "1          11      -1.001681  not sig  \n",
      "2         122      -0.685994  not sig  \n",
      "3         125      -0.576308  not sig  \n",
      "4         126      -0.490536  not sig  \n",
      "5         130      -0.589506  not sig  \n",
      "6         143      -0.670206  not sig  \n",
      "7         147      -0.542364  not sig  \n",
      "8         158       0.127804  not sig  \n",
      "9         189       0.048035  not sig  \n"
     ]
    }
   ],
   "source": [
    "# Apply event name mapping to results DataFrame\n",
    "def apply_event_mapping_to_results(df, event_mapping):\n",
    "    \"\"\"\n",
    "    Apply event name mapping to the results DataFrame\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        print(\"DataFrame is empty, nothing to map\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Original DataFrame shape: {df.shape}\")\n",
    "    print(f\"Original events: {sorted(df['Event name'].unique())}\")\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    mapped_df = df.copy()\n",
    "    \n",
    "    # Apply the mapping\n",
    "    mapped_df['Event name'] = mapped_df['Event name'].map(event_mapping).fillna(mapped_df['Event name'])\n",
    "    \n",
    "    # Remove rows where events should be skipped (mapped to None)\n",
    "    original_size = len(mapped_df)\n",
    "    mapped_df = mapped_df[mapped_df['Event name'].notna()]\n",
    "    skipped_count = original_size - len(mapped_df)\n",
    "    \n",
    "    print(f\"Mapped DataFrame shape: {mapped_df.shape}\")\n",
    "    print(f\"Mapped events: {sorted(mapped_df['Event name'].unique())}\")\n",
    "    print(f\"Rows skipped (unmapped events): {skipped_count}\")\n",
    "    \n",
    "    return mapped_df\n",
    "\n",
    "# Apply the mapping to your results\n",
    "print(\"Applying event name mapping to results...\")\n",
    "mapped_results_df = apply_event_mapping_to_results(results_df, event_mapping)\n",
    "\n",
    "print(f\"\\nðŸ“Š Mapping Summary:\")\n",
    "print(f\"- Original results: {results_df.shape[0]} rows\")\n",
    "print(f\"- Mapped results: {mapped_results_df.shape[0]} rows\") \n",
    "print(f\"- Events after mapping: {sorted(mapped_results_df['Event name'].unique())}\")\n",
    "\n",
    "# Show a sample of the mapped results\n",
    "if not mapped_results_df.empty:\n",
    "    print(f\"\\nSample of mapped results:\")\n",
    "    print(mapped_results_df[['Alt_Recording', 'Event name', 'Unit number', 'Event Z-Score', 'sig']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1de7bc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating analysis-ready dataset...\n",
      "\n",
      "Final analysis-ready dataset:\n",
      "- Shape: (1808, 10)\n",
      "- Events included: ['high_comp_lose', 'high_comp_win', 'low_comp_lose', 'low_comp_win']\n",
      "- Recordings: 29\n",
      "- Units: 231\n",
      "\n",
      "Event counts:\n",
      "  high_comp_lose: 475\n",
      "  low_comp_win: 458\n",
      "  low_comp_lose: 449\n",
      "  high_comp_win: 426\n",
      "\n",
      "Significance distribution:\n",
      "  not sig: 1679\n",
      "  increase: 114\n",
      "  decrease: 15\n",
      "\n",
      "âœ… Your mapped dataset is ready!\n",
      "Use 'analysis_ready_df' for your analyses - it contains the correctly mapped event names.\n"
     ]
    }
   ],
   "source": [
    "# Save the mapped results and create analysis-ready data\n",
    "print(\"Creating analysis-ready dataset...\")\n",
    "\n",
    "# Remove 'tie' events if you want to skip them for your analysis\n",
    "analysis_ready_df = mapped_results_df[mapped_results_df['Event name'] != 'tie'].copy()\n",
    "\n",
    "print(f\"\\nFinal analysis-ready dataset:\")\n",
    "print(f\"- Shape: {analysis_ready_df.shape}\")\n",
    "print(f\"- Events included: {sorted(analysis_ready_df['Event name'].unique())}\")\n",
    "print(f\"- Recordings: {analysis_ready_df['Alt_Recording'].nunique()}\")\n",
    "print(f\"- Units: {analysis_ready_df['Unit number'].nunique()}\")\n",
    "\n",
    "# Show event counts\n",
    "print(f\"\\nEvent counts:\")\n",
    "event_counts = analysis_ready_df['Event name'].value_counts()\n",
    "for event, count in event_counts.items():\n",
    "    print(f\"  {event}: {count}\")\n",
    "\n",
    "# Show significance distribution\n",
    "print(f\"\\nSignificance distribution:\")\n",
    "sig_counts = analysis_ready_df['sig'].value_counts()\n",
    "for sig_type, count in sig_counts.items():\n",
    "    print(f\"  {sig_type}: {count}\")\n",
    "\n",
    "# Optional: Save the mapped results\n",
    "save_results = False  # Change to True if you want to save\n",
    "if save_results:\n",
    "    output_path = r'C:\\Users\\thoma\\Code\\ResearchCode\\diff_fam_social_memory_ephys\\other_peoples_sutff\\Thomas\\reward_comp\\testing_eventdict_creation\\mapped_zscore_results.csv'\n",
    "    analysis_ready_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nðŸ’¾ Results saved to: {output_path}\")\n",
    "\n",
    "print(f\"\\nâœ… Your mapped dataset is ready!\")\n",
    "print(f\"Use 'analysis_ready_df' for your analyses - it contains the correctly mapped event names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad15d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_df = analysis_ready_df[analysis_ready_df['sig'] != 'not sig']\n",
    "units_df = units_df.drop([\"Global Pre-event M\", \"Global Pre-event SD\", \"Event M\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c301fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant units for high_comp_win: 21\n"
     ]
    }
   ],
   "source": [
    "df = units_df.copy()\n",
    "event_df = df[df['Event name'] == 'high_comp_win']\n",
    "\n",
    "significant_units = event_df['Unit number'].unique()\n",
    "print(f\"Number of significant units for high_comp_win: {len(significant_units)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "723ee252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Alt_Recording",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "SP_Recording",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Event name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Unit number",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Event Z-Score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sig",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Event count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "361d66a3-87f8-4e59-aaa3-7fe4a6740b9d",
       "rows": [
        [
         "37",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec",
         "high_comp_win",
         "232",
         "2.061272905863388",
         "increase",
         "8"
        ],
        [
         "81",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3",
         "20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec",
         "low_comp_win",
         "232",
         "1.749791666755143",
         "increase",
         "5"
        ],
        [
         "90",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec",
         "high_comp_lose",
         "46",
         "1.768666000151225",
         "increase",
         "8"
        ],
        [
         "97",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec",
         "high_comp_lose",
         "68",
         "1.8311675519117858",
         "increase",
         "8"
        ],
        [
         "102",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec",
         "high_comp_win",
         "50",
         "2.6733575215890353",
         "increase",
         "1"
        ],
        [
         "104",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec",
         "high_comp_win",
         "52",
         "2.1877094062094637",
         "increase",
         "1"
        ],
        [
         "106",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec",
         "high_comp_win",
         "61",
         "1.7270830079817594",
         "increase",
         "1"
        ],
        [
         "107",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec",
         "high_comp_win",
         "68",
         "1.9720265943665387",
         "increase",
         "1"
        ],
        [
         "124",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec",
         "low_comp_win",
         "52",
         "1.7194719589827057",
         "increase",
         "6"
        ],
        [
         "127",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4",
         "20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec",
         "low_comp_win",
         "68",
         "10.79919325486438",
         "increase",
         "6"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alt_Recording</th>\n",
       "      <th>SP_Recording</th>\n",
       "      <th>Event name</th>\n",
       "      <th>Unit number</th>\n",
       "      <th>Event Z-Score</th>\n",
       "      <th>sig</th>\n",
       "      <th>Event count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec</td>\n",
       "      <td>high_comp_win</td>\n",
       "      <td>232</td>\n",
       "      <td>2.061273</td>\n",
       "      <td>increase</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec</td>\n",
       "      <td>low_comp_win</td>\n",
       "      <td>232</td>\n",
       "      <td>1.749792</td>\n",
       "      <td>increase</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec</td>\n",
       "      <td>high_comp_lose</td>\n",
       "      <td>46</td>\n",
       "      <td>1.768666</td>\n",
       "      <td>increase</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec</td>\n",
       "      <td>high_comp_lose</td>\n",
       "      <td>68</td>\n",
       "      <td>1.831168</td>\n",
       "      <td>increase</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec</td>\n",
       "      <td>high_comp_win</td>\n",
       "      <td>50</td>\n",
       "      <td>2.673358</td>\n",
       "      <td>increase</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec</td>\n",
       "      <td>high_comp_win</td>\n",
       "      <td>52</td>\n",
       "      <td>2.187709</td>\n",
       "      <td>increase</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec</td>\n",
       "      <td>high_comp_win</td>\n",
       "      <td>61</td>\n",
       "      <td>1.727083</td>\n",
       "      <td>increase</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec</td>\n",
       "      <td>high_comp_win</td>\n",
       "      <td>68</td>\n",
       "      <td>1.972027</td>\n",
       "      <td>increase</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec</td>\n",
       "      <td>low_comp_win</td>\n",
       "      <td>52</td>\n",
       "      <td>1.719472</td>\n",
       "      <td>increase</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4</td>\n",
       "      <td>20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec</td>\n",
       "      <td>low_comp_win</td>\n",
       "      <td>68</td>\n",
       "      <td>10.799193</td>\n",
       "      <td>increase</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Alt_Recording  \\\n",
       "37   20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
       "81   20230612_101430_standard_comp_to_training_D1_subj_1-3   \n",
       "90   20230612_101430_standard_comp_to_training_D1_subj_1-4   \n",
       "97   20230612_101430_standard_comp_to_training_D1_subj_1-4   \n",
       "102  20230612_101430_standard_comp_to_training_D1_subj_1-4   \n",
       "104  20230612_101430_standard_comp_to_training_D1_subj_1-4   \n",
       "106  20230612_101430_standard_comp_to_training_D1_subj_1-4   \n",
       "107  20230612_101430_standard_comp_to_training_D1_subj_1-4   \n",
       "124  20230612_101430_standard_comp_to_training_D1_subj_1-4   \n",
       "127  20230612_101430_standard_comp_to_training_D1_subj_1-4   \n",
       "\n",
       "                                                                    SP_Recording  \\\n",
       "37   20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
       "81   20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec   \n",
       "90   20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec   \n",
       "97   20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec   \n",
       "102  20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec   \n",
       "104  20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec   \n",
       "106  20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec   \n",
       "107  20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec   \n",
       "124  20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec   \n",
       "127  20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec   \n",
       "\n",
       "         Event name Unit number  Event Z-Score       sig  Event count  \n",
       "37    high_comp_win         232       2.061273  increase            8  \n",
       "81     low_comp_win         232       1.749792  increase            5  \n",
       "90   high_comp_lose          46       1.768666  increase            8  \n",
       "97   high_comp_lose          68       1.831168  increase            8  \n",
       "102   high_comp_win          50       2.673358  increase            1  \n",
       "104   high_comp_win          52       2.187709  increase            1  \n",
       "106   high_comp_win          61       1.727083  increase            1  \n",
       "107   high_comp_win          68       1.972027  increase            1  \n",
       "124    low_comp_win          52       1.719472  increase            6  \n",
       "127    low_comp_win          68      10.799193  increase            6  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "units_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0df6fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using os to use functions from this py file C:\\Users\\thoma\\Code\\ResearchCode\\diff_fam_social_memory_ephys\\other_peoples_sutff\\Thomas\\reward_comp\\venn_upset_plot_creation.py\n",
    "import os\n",
    "import sys\n",
    "module_path = r\"C:/Users/thoma/Code/ResearchCode/diff_fam_social_memory_ephys/other_peoples_sutff/Thomas/reward_comp\"\n",
    "sys.path.append(module_path)\n",
    "\n",
    "from venn_upset_plot_creation import create_venn_diagram, analyze_event_overlap, create_overlap_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f7586305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE 1: Alone Rewarded vs Win vs Lose Events\n",
      "Significant units for increase: []\n",
      "Significant units for decrease: []\n",
      "All significant units: []\n",
      "alone_rewarded: 0 significant units\n",
      "Significant units for increase: []\n",
      "Significant units for decrease: []\n",
      "All significant units: []\n",
      "win: 0 significant units\n",
      "Significant units for increase: []\n",
      "Significant units for decrease: []\n",
      "All significant units: []\n",
      "lose: 0 significant units\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Recording'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\thoma\\miniconda3\\envs\\ephys_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Recording'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEXAMPLE 1: Alone Rewarded vs Win vs Lose Events\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m events_to_compare \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malone_rewarded\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlose\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_venn_diagram\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents_to_compare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignificance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mboth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAll Significant Units\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[1;32mC:\\Users/thoma/Code/ResearchCode/diff_fam_social_memory_ephys/other_peoples_sutff/Thomas/reward_comp\\venn_upset_plot_creation.py:353\u001b[0m, in \u001b[0;36mcreate_venn_diagram\u001b[1;34m(df, compare_events, significance_type, title)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_venn_diagram\u001b[39m(df, compare_events, significance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m'\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Legacy function name - redirects to create_overlap_visualization\"\"\"\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_overlap_visualization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompare_events\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignificance_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Users/thoma/Code/ResearchCode/diff_fam_social_memory_ephys/other_peoples_sutff/Thomas/reward_comp\\venn_upset_plot_creation.py:78\u001b[0m, in \u001b[0;36mcreate_overlap_visualization\u001b[1;34m(df, compare_events, significance_type, title)\u001b[0m\n\u001b[0;32m     75\u001b[0m event_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m event]\n\u001b[0;32m     76\u001b[0m unique_identifiers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rec, unit \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mevent_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRecording\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, event_dicts[event]):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# Extract subject ID using regex to find \"subj_X-Y\" pattern\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     subject_match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubj_[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md-]+\u001b[39m\u001b[38;5;124m'\u001b[39m, rec)\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m subject_match:\n",
      "File \u001b[1;32mc:\\Users\\thoma\\miniconda3\\envs\\ephys_env\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\thoma\\miniconda3\\envs\\ephys_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Recording'"
     ]
    }
   ],
   "source": [
    "# Example 1: Compare 'alone_rewarded', 'win', and 'lose' events (your specific example)\n",
    "print(\"EXAMPLE 1: Alone Rewarded vs Win vs Lose Events\")\n",
    "events_to_compare = ['alone_rewarded', 'win', 'lose']\n",
    "_ = create_venn_diagram(units_df, events_to_compare, significance_type='both', \n",
    "                        title=\"All Significant Units\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
