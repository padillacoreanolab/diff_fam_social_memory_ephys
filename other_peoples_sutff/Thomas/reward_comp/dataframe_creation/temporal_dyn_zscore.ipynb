{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5797dd0a",
   "metadata": {},
   "source": [
    "# This notebook creates the dataframe to look at the z-score from a temporal perspective\n",
    "\n",
    "### What we have\n",
    "- We already have the dataframe of the z-scores for every unit. This doesn't tell us much about the time though\n",
    "- we have old spike collection that gives us the neurons spikes across time and the event dicts and their times\n",
    "\n",
    "### What we'll do\n",
    "- First, break up these events by time they happened, for example 1/3's where we just create the z-score dataframe for the events that fall into the first 3rd of the recording time, then the 2/3, then 3/3 we then plot the z-scored during these 3 time periods seperately for events to see if it changes\n",
    "- second, use gaussian smoothing filter to see individual neurons continuously changing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb421392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spike.spike_analysis.spike_collection as sc\n",
    "import spike.spike_analysis.spike_recording as sr\n",
    "import spike.spike_analysis.firing_rate_calculations as fr\n",
    "import spike.spike_analysis.normalization as norm\n",
    "import spike.spike_analysis.single_cell as single_cell\n",
    "import spike.spike_analysis.spike_collection as collection\n",
    "import spike.spike_analysis.zscoring as zscoring\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as scipy\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import fftconvolve\n",
    "import os\n",
    "import behavior.boris_extraction as boris\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262ef7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)  # 0 means unlimited in newer pandas versions\n",
    "\n",
    "# Show all rows\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Donâ€™t truncate column contents\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Expand the display to the full width of the screen\n",
    "pd.set_option(\"display.width\", 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b33a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_collection_json_path = r'C:\\Users\\thoma\\Code\\ResearchCode\\diff_fam_social_memory_ephys\\spike_collection.json\\spike_collection.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4698a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = sc.SpikeCollection.load_collection(spike_collection_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573cce6",
   "metadata": {},
   "source": [
    "#### Quick look at the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5248eb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique event names: ['alone_rewarded', 'alone_rewarded_baseline', 'high_comp', 'high_comp_lose', 'high_comp_lose_baseline', 'high_comp_win', 'high_comp_win_baseline', 'lose', 'low_comp', 'low_comp_lose', 'low_comp_lose_baseline', 'low_comp_win', 'low_comp_win_baseline', 'overall_pretone', 'win']\n"
     ]
    }
   ],
   "source": [
    "rec_events = sp.recordings[0].event_dict\n",
    "\n",
    "# get unique event names from rec_events dictionary\n",
    "event_names = list(rec_events.keys())\n",
    "print(\"Unique event names:\", event_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39cd02",
   "metadata": {},
   "source": [
    "#### Quick Look at all the recording names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd92259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique recording names: ['20230612_101430_standard_comp_to_training_D1_subj_1-3_t3b3L_box2_merged.rec', '20230612_101430_standard_comp_to_training_D1_subj_1-4_t4b2L_box1_merged.rec', '20230612_112630_standard_comp_to_training_D1_subj_1-1_t1b3L_box2_merged.rec', '20230612_112630_standard_comp_to_training_D1_subj_1-2_t2b2L_box1_merged.rec', '20230613_105657_standard_comp_to_training_D2_subj_1-1_t1b2L_box1_merged.rec', '20230613_105657_standard_comp_to_training_D2_subj_1-4_t4b3L_box2_merged.rec', '20230614_114041_standard_comp_to_training_D3_subj_1-1_t1b3L_box1_merged.rec', '20230614_114041_standard_comp_to_training_D3_subj_1-2_t2b2L_box2_merged.rec', '20230616_111904_standard_comp_to_training_D4_subj_1-2_t2b2L_box2_merged.rec', '20230616_111904_standard_comp_to_training_D4_subj_1-4_t4b3L_box1_merged.rec', '20230617_115521_standard_comp_to_omission_D1_subj_1-1_t1b3L_box1_merged.rec', '20230617_115521_standard_comp_to_omission_D1_subj_1-2_t2b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1-1_t1b2L_box2_merged.rec', '20230618_100636_standard_comp_to_omission_D2_subj_1-4_t4b3L_box1_merged.rec', '20230619_115321_standard_comp_to_omission_D3_subj_1-4_t3b3L_box2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-1_t1b2L_box_2_merged.rec', '20230620_114347_standard_comp_to_omission_D4_subj_1-2_t3b3L_box_1_merged.rec', '20230621_111240_standard_comp_to_omission_D5_subj_1-4_t3b3L_box1_merged.rec', '20240317_151922_long_comp_subj_3-1_t6b6_merged.rec', '20240317_151922_long_comp_subj_3-3_t5b5_merged.rec', '20240317_172017_long_comp_subj_4-2_t6b6_merged.rec', '20240317_172017_long_comp_subj_4-3_t5b5_merged.rec', '20240318_143819_long_comp_subj_3-3_t6b6_merged.rec', '20240318_143819_long_comp_subj_3-4_t5b5_merged.rec', '20240318_170933_long_comp_subj_4-3_t6b6_merged.rec', '20240319_160457_long_comp_subj_4-2_t5b5_merged.rec', '20240320_114629_long_comp_subj_5-3_t6b6_merged.rec', '20240320_142408_alone_comp_subj_3-1_t6b6_merged.rec', '20240320_142408_alone_comp_subj_3-3_t5b5_merged.rec', '20240320_171038_alone_comp_subj_4-2_t6b6_merged.rec', '20240320_171038_alone_comp_subj_4-3_t5b5_merged.rec', '20240321_114851_long_comp_subj_5-3_t5b5_merged.rec', '20240322_120625_alone_comp_subj_3-3_t6b6_merged.rec', '20240322_120625_alone_comp_subj_3-4_t5b5_merged.rec', '20240322_160946_alone_comp_subj_4-3_t6b6_merged.rec', '20240323_122227_alone_comp_subj_5-2_t6b6_merged.rec', '20240323_144517_alone_comp_subj_3-1_t5b5_merged.rec', '20240323_144517_alone_comp_subj_3-4_t6b6_merged.rec', '20240323_165815_alone_comp_subj_4-2_t5b5_merged.rec']\n"
     ]
    }
   ],
   "source": [
    "# printing unique recording names\n",
    "recording_names = [rec.name for rec in sp.recordings]\n",
    "print(\"Unique recording names:\", recording_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "607ca66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F7E094280>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F7E076AD0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F7CD35720>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F7E0764D0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F7E03E9B0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F7E03C9A0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F021E1C00>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F7E076620>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F7E075AE0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F7CD375E0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F021E17E0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F022146A0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214790>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214640>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214760>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214A90>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214BB0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214820>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214850>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214490>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214670>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F022145E0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F022143D0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214310>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F022148E0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F022144F0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F022141F0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F022141C0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214B20>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214940>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F022145B0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214220>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214A30>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214190>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214550>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F022150C0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214880>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F022142E0>, <spike.spike_analysis.spike_recording.SpikeRecording object at 0x0000023F02214A60>]\n"
     ]
    }
   ],
   "source": [
    "# list of recording objects in sp\n",
    "recs = sp.recordings\n",
    "print(recs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ade057e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[210  65  74  73  85  85 196 210  51 211  73 192  65 211 196 126  22  85\n",
      "  34  65]\n"
     ]
    }
   ],
   "source": [
    "print(recs[0].unit_array[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_bins(sigma_ms: float, timebin_ms: float, support: float = 3.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Discrete symmetric Gaussian in *bins*, normalized so that\n",
    "    convolving counts/bin yields *Hz* directly.\n",
    "    \"\"\"\n",
    "    sigma_bins = float(sigma_ms) / float(timebin_ms)\n",
    "    half = int(np.ceil(support * sigma_bins))\n",
    "    n = np.arange(-half, half + 1, dtype=float)\n",
    "    k = np.exp(-0.5 * (n / sigma_bins) ** 2)\n",
    "\n",
    "    dt = timebin_ms / 1000.0  # seconds per bin\n",
    "    k /= (k.sum() * dt)       # ensures output is in Hz\n",
    "    return k\n",
    "\n",
    "\n",
    "def smooth_rate_fft(train_counts: np.ndarray, kernel_bins: np.ndarray) -> np.ndarray:\n",
    "    pad = len(kernel_bins) // 2\n",
    "    x = np.pad(train_counts.astype(float), pad, mode='reflect')\n",
    "    rate_hz = fftconvolve(x, kernel_bins, mode='same')[pad:-pad]\n",
    "    return rate_hz\n",
    "\n",
    "def smooth_rate_ndimage(train_counts: np.ndarray, sigma_ms: float, timebin_ms: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convenience: gaussian_filter1d returns counts/bin; divide by dt to get Hz.\n",
    "    \"\"\"\n",
    "    sigma_bins = float(sigma_ms) / float(timebin_ms)\n",
    "    counts_sm = gaussian_filter1d(train_counts.astype(float),\n",
    "                                  sigma=sigma_bins, mode='reflect', truncate=3.0)\n",
    "    dt = timebin_ms / 1000.0\n",
    "    return counts_sm / dt\n",
    "\n",
    "\n",
    "def zscore_from_baseline(rate_hz: np.ndarray, t_rel_ms: np.ndarray,\n",
    "                         baseline_ms=(-1500.0, -250.0), robust=False) -> np.ndarray:\n",
    "    b = (t_rel_ms >= baseline_ms[0]) & (t_rel_ms <= baseline_ms[1])\n",
    "    bvals = rate_hz[b]\n",
    "    mu = np.mean(bvals) if bvals.size else 0.0\n",
    "    if robust and bvals.size:\n",
    "        med = np.median(bvals)\n",
    "        mad = np.median(np.abs(bvals - med))\n",
    "        sd = 1.4826 * mad if mad > 0 else np.std(bvals, ddof=1)\n",
    "    else:\n",
    "        sd = np.std(bvals, ddof=1) if bvals.size else 0.0\n",
    "    sd = sd if sd > 1e-12 else 1e-12\n",
    "    return (rate_hz - mu) / sd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411544df",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs:\n",
    "One part we're specifying here the input values we want for the gaussian kernel aka\n",
    " - sigma_ms, our standard deviation in ms\n",
    " - timebin_ms, the size of our bins in ms\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def compute_unit_ztrials_ms(rec, unit_id, event_windows_ms,\n",
    "                            win_ms=(-2000.0, 8000.0),\n",
    "                            timebin_ms=5.0,\n",
    "                            sigma_ms=75.0,\n",
    "                            baseline_ms=(-1500.0, -250.0),\n",
    "                            use_fft=True, robust=False):\n",
    "    \"\"\"\n",
    "    rec.unit_timestamps[unit_id] must be in SAMPLES (your class format).\n",
    "    event_windows_ms: array-like of [start_ms, end_ms] pairs (same as rec.event_dict[...] entries).\n",
    "    Returns: t_rel_ms (T,), z_trials (n_events, T), rate_trials_hz (n_events, T)\n",
    "    \"\"\"\n",
    "    # --- build the common time grid relative to onset\n",
    "    T = int(np.floor((win_ms[1] - win_ms[0]) / timebin_ms))\n",
    "    t_rel_ms = np.arange(T) * timebin_ms + win_ms[0]\n",
    "\n",
    "    # --- choose smoother\n",
    "    if use_fft:\n",
    "        k = gaussian_kernel_bins(sigma_ms, timebin_ms, support=3.0)\n",
    "        smooth = lambda counts: smooth_rate_fft(counts, k)\n",
    "    else:\n",
    "        smooth = lambda counts: smooth_rate_ndimage(counts, sigma_ms, timebin_ms)\n",
    "\n",
    "    # --- spikes for this unit (samples -> ms)\n",
    "    spikes_samples = rec.unit_timestamps[str(unit_id)] if str(unit_id) in rec.unit_timestamps else rec.unit_timestamps[unit_id]\n",
    "    spikes_ms = spikes_samples * (1000.0 / rec.sampling_rate)\n",
    "\n",
    "    z_trials = []\n",
    "    rate_trials = []\n",
    "\n",
    "    for (start_ms, _end_ms) in np.asarray(event_windows_ms):\n",
    "        t0 = start_ms + win_ms[0]\n",
    "        t1 = start_ms + win_ms[1]\n",
    "        # make_bin edges\n",
    "        edges = np.arange(t0, t1 + timebin_ms, timebin_ms)\n",
    "        counts, _ = np.histogram(spikes_ms, bins=edges)  # counts/bin\n",
    "        # smooth -> Hz\n",
    "        rate_hz = smooth(counts)\n",
    "        # z-score on relative axis\n",
    "        z = zscore_from_baseline(rate_hz, t_rel_ms, baseline_ms, robust=robust)\n",
    "        z_trials.append(z)\n",
    "        rate_trials.append(rate_hz)\n",
    "\n",
    "    return t_rel_ms, np.vstack(z_trials), np.vstack(rate_trials)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ephys_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
